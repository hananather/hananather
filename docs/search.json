[
  {
    "objectID": "CLAUDE.html",
    "href": "CLAUDE.html",
    "title": "CLAUDE.md",
    "section": "",
    "text": "This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n\nThis is a Quarto-based personal blog and portfolio website for Hanan Ather. The site uses Quarto to generate static HTML from Markdown and Jupyter notebook sources.\n\n\n\n# Preview the site locally with live reload\nquarto preview\n\n# Build the site (outputs to docs/ directory)\nquarto render\n\n# Build a specific file\nquarto render index.qmd\nquarto render posts/introduction-to-llms/index.ipynb\n\n\n\n\n**_quarto.yml**: Main configuration file for the Quarto website\nindex.qmd: Homepage with bio, CV link, and course notes\nposts/: Blog posts directory (supports .qmd and .ipynb files)\n\nindex.qmd: Blog listing page\n\nprinciples.qmd: About/principles page\ndocs/: Generated static site output (do not edit directly)\nwebsite_docs/: PDF documents and static assets\n\n\n\n\nThe site is configured as a Quarto website project that: - Outputs to docs/ directory for GitHub Pages hosting - Uses Cosmo (light) and Cyborg (dark) themes - Has a simple navbar with Home, My Writing, and About sections - Uses freeze: auto for computational documents to cache outputs\n\n\n\n\nAdding a new blog post:\n\nCreate a new directory under posts/ with your post name\nAdd either an index.qmd (Markdown) or index.ipynb (Jupyter notebook)\nInclude YAML frontmatter with title, date, author, description, and categories\nRun quarto render to build\n\nModifying site structure:\n\nEdit _quarto.yml for navigation, themes, or global settings\nIndividual page settings can be overridden in their YAML frontmatter\n\nWorking with Jupyter notebooks:\n\nNotebooks in posts/ are automatically converted to HTML\nUse freeze: auto to cache computational outputs\nInclude images as attachments or separate files\n\n\n\n\n\nThe repository uses GitHub Pages deployed from the docs/ folder on the main branch. Always render the site before committing changes that affect content.\n\n\n\n\nThe docs/ directory is the build output - never edit these files directly\nAll content changes should be made to source files (.qmd, .ipynb)\nThe site uses responsive Bootstrap themes with automatic light/dark mode switching\nPDF documents in website_docs/ are directly linked from the homepage"
  },
  {
    "objectID": "CLAUDE.html#project-overview",
    "href": "CLAUDE.html#project-overview",
    "title": "CLAUDE.md",
    "section": "",
    "text": "This is a Quarto-based personal blog and portfolio website for Hanan Ather. The site uses Quarto to generate static HTML from Markdown and Jupyter notebook sources."
  },
  {
    "objectID": "CLAUDE.html#build-commands",
    "href": "CLAUDE.html#build-commands",
    "title": "CLAUDE.md",
    "section": "",
    "text": "# Preview the site locally with live reload\nquarto preview\n\n# Build the site (outputs to docs/ directory)\nquarto render\n\n# Build a specific file\nquarto render index.qmd\nquarto render posts/introduction-to-llms/index.ipynb"
  },
  {
    "objectID": "CLAUDE.html#project-structure",
    "href": "CLAUDE.html#project-structure",
    "title": "CLAUDE.md",
    "section": "",
    "text": "**_quarto.yml**: Main configuration file for the Quarto website\nindex.qmd: Homepage with bio, CV link, and course notes\nposts/: Blog posts directory (supports .qmd and .ipynb files)\n\nindex.qmd: Blog listing page\n\nprinciples.qmd: About/principles page\ndocs/: Generated static site output (do not edit directly)\nwebsite_docs/: PDF documents and static assets"
  },
  {
    "objectID": "CLAUDE.html#key-configuration",
    "href": "CLAUDE.html#key-configuration",
    "title": "CLAUDE.md",
    "section": "",
    "text": "The site is configured as a Quarto website project that: - Outputs to docs/ directory for GitHub Pages hosting - Uses Cosmo (light) and Cyborg (dark) themes - Has a simple navbar with Home, My Writing, and About sections - Uses freeze: auto for computational documents to cache outputs"
  },
  {
    "objectID": "CLAUDE.html#development-workflow",
    "href": "CLAUDE.html#development-workflow",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Adding a new blog post:\n\nCreate a new directory under posts/ with your post name\nAdd either an index.qmd (Markdown) or index.ipynb (Jupyter notebook)\nInclude YAML frontmatter with title, date, author, description, and categories\nRun quarto render to build\n\nModifying site structure:\n\nEdit _quarto.yml for navigation, themes, or global settings\nIndividual page settings can be overridden in their YAML frontmatter\n\nWorking with Jupyter notebooks:\n\nNotebooks in posts/ are automatically converted to HTML\nUse freeze: auto to cache computational outputs\nInclude images as attachments or separate files"
  },
  {
    "objectID": "CLAUDE.html#git-workflow",
    "href": "CLAUDE.html#git-workflow",
    "title": "CLAUDE.md",
    "section": "",
    "text": "The repository uses GitHub Pages deployed from the docs/ folder on the main branch. Always render the site before committing changes that affect content."
  },
  {
    "objectID": "CLAUDE.html#important-notes",
    "href": "CLAUDE.html#important-notes",
    "title": "CLAUDE.md",
    "section": "",
    "text": "The docs/ directory is the build output - never edit these files directly\nAll content changes should be made to source files (.qmd, .ipynb)\nThe site uses responsive Bootstrap themes with automatic light/dark mode switching\nPDF documents in website_docs/ are directly linked from the homepage"
  },
  {
    "objectID": "website_docs/My Guiding Principles 1de85548209844fbaf58dcc549713ccc.html",
    "href": "website_docs/My Guiding Principles 1de85548209844fbaf58dcc549713ccc.html",
    "title": "My Guiding Principles",
    "section": "",
    "text": "My Guiding Principles\n\nWork hard. Extreme people get extreme results. Hard work compounds like interest, the earlier you do it, the more time you have for the benefits to pay off.\nHumans have been writing down their best ideas for 5,000 years. Read them. Make friends with smart dead people. Seneca, Feynman, Montaigne, Charlie Munger —whoever interests you. Read their writing. Steal their ideas. They don’t need them anymore.\nAlways have the courage to be disliked for your true self rather than being liked for someone you are not.\nBurnout ≠ Working Too Much. Burnout stems from work that doesn’t align with your true values. Its the result of meaningless work.\nSet ambitious goals but focus all your energy on actions you can control. Worrying about the uncontrollables is almost always a unproductive.\nDon’t divide your attention. Focusing on one thing yields increasing returns for each unit of effort.\nLearning should transform how you act. If an idea doesn’t influence your actions, what was the purpose of learning it in the first place?\nAlways engage with and understand the very best version of your opponent’s argument.\nTake the time to listen to smart people with whom you disagree.\nDo the unpleasant task first. Don’t multitask. Always work harder than you think you should. Hard work is just a habit.\nFocus on what is present in front of you, and pay full attention to it.\nDon’t fall into the trap of endless dissatisfaction. You can be highly ambitious and still appreciate the present.\nStay detached from things that are not up to you.\n\n“Not being able to govern events, I govern myself, and if they will not adapt to me, I adapt to them.” -Montaigne, Of Presumption (1580)\n\n\n“Human life the Stoics appear to have considered as a game of great skill; in which, however, there was a mixture of chance, or of what is vulgarly understood to be chance…. If we placed our happiness in winning the stake, we placed it in what depended upon causes beyond our power, and out of our direction. We necessarily exposed ourselves to perpetual fear and uneasiness, and frequently to grievous and mortifying disappointments. If we placed it in playing well, in playing fairly, in playing wisely and skillfully; in the propriety of our own conduct in short; we placed it in what, by proper discipline, education, and attention, might be altogether in our own power, and under our own direction. Our happiness was perfectly secure, and beyond the reach of fortune.” -Adam Smith, The Theory of Moral Sentiments (1759)\n\nDo as much as you can every day; figure out what your primary focus is and make progress on that every day.\n“Hard work” is not the same as “valuable work.”\nPeople systematically undervalue their time. Try to overvalue your time.\nAlways be high integrity, even when it costs you. The shortcuts aren’t worth it.\nSurround yourself with truly excellent people. Top 1% in some important way.\nWe don’t react to events; we react to our judgments about them, and the judgments are up to us. There is the objective reality of what happens, and there is the subjective reality of how what happened is seen, interpreted, made meaningful. These ****three quotes span 1600 years and basically echo this underlying idea:\n\n“If any external thing causes you distress, it is not the thing itself that troubles you, but your own judgment about it. And this you have the power to eliminate now.” -Marcus Aurelius, Meditations (161 AD)\n\n\n“It is not what things are objectively and in themselves, but what they are for us, in our way of looking at them, that makes us happy or the reverse.” -Schopenhauer, The Wisdom of Life (1851)\n\n\n”There is nothing either good or bad, but thinking makes it so.”\n-Shakespear, Hamlet (1603)\n\nMake deliberate decisions. where to live, what kind of people to surround yourself with, what habits to cultivate. These choices will define you. Living “authentically” means being honest with yourself. “Am I happy with the direction I’m going? Does it align with my values?” If not, remember you have the agency to change course. If your definition of a good life differs from societal or family expectations, have the courage to pursue what resonates with you. Own your life."
  },
  {
    "objectID": "posts/introduction-to-llms/index.html",
    "href": "posts/introduction-to-llms/index.html",
    "title": "Introduction to LLMs",
    "section": "",
    "text": "How many of you have had a conversation like this with ChatGPT recently?\n\nUser: \"Analyze the trends in our Q3 survey data. We had 15,000 responses \nabout housing affordability across different provinces.\"\n\nChatGPT: \"Based on your description, here are some key trends I'd expect \nto see in Q3 housing affordability data:\n\n1. Regional variations likely show BC and Ontario with highest concerns\n2. Age demographics probably reveal millennials most affected\n3. Income brackets under $75K likely report significant challenges\n4. Urban vs rural differences may show urban areas more impacted\n\nTo get specific insights, I'd recommend looking at cross-tabulations \nbetween income, location, and reported affordability stress levels...\"\n\nThis highlights a fundamental challenge we face in 2025: LLMs are incredibly smart, but they’re designed for human conversation, not system integration\nIn our daily work at StatCan, we don’t just need insights we need those insights to flow into our data pipelines, our reporting systems, our analysis workflows. We need to bridge this gap between conversational AI and operational systems.\nAPI versions of these same LLMs can eliminate this translation step entirely. Instead of getting human-readable text that you then have to interpret and act upon, you can get: - Structured data that flows directly into your analysis tools - Function calls that automatically trigger the right actions in your systems - Formatted outputs that match your reporting requirements exactly\nBefore: \nAsk ChatGPT question → 2. Read response → 3. Interpret manually → 4. Write code/queries → 5. Execute actions → 6. Format results\n\n\n\nimage.png\n\n\nAfter: Send API request → 2. LLM returns structured data + triggers actions → 3. Results automatically formatted and delivered\nThe key insight is this: The same AI capabilities you use in chat can be repurposed as intelligent middleware in your data systems. Let me show you exactly what this looks like in practice, starting with a simple example that will make the concept crystal clear."
  },
  {
    "objectID": "posts/introduction-to-llms/index.html#conversation-state",
    "href": "posts/introduction-to-llms/index.html#conversation-state",
    "title": "Introduction to LLMs",
    "section": "Conversation state",
    "text": "Conversation state\nThis naturally leads us to the idea of how to manage conversation state during a model interaction. There are a few ways to manage conversation state, which is important for preserving information across multiple messages or turns in a conversation.\n\nManually manage the conversation state\nGenerally for most APIs each text generation request is independent and stateless (unless we are specific APIs like the Assistants API), but we can still implement multi-turn conversations by providing additional messages as parameters to our text generation request.\n\n\nShow code\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"What does Statistics Canada do?\"},\n        {\"role\": \"assistant\", \"content\": \"Statistics Canada is the national statistical office that collects, analyzes, and publishes statistical information about Canada's economy, society, and environment.\"},\n        {\"role\": \"user\", \"content\": \"What are their main survey types?\"},\n    ],\n)\n\nprint(response.choices[0].message.content)\n\n\nStatistics Canada conducts various types of surveys, which can be broadly categorized into the following main types:\n\n1. **Household Surveys**: These surveys gather data on the population, including labor force participation, income, health, education, and housing. Examples include the Labour Force Survey and the Census of Population.\n\n2. **Business Surveys**: These surveys collect information on the economic activities of businesses, including production, sales, and employment. Examples include the Annual Survey of Manufacturing and the Canadian Survey on Business Conditions.\n\n3. **Agricultural Surveys**: These surveys collect data on various aspects of agriculture, including crop production, livestock, and farm management practices. An example is the Census of Agriculture.\n\n4. **Census**: The Census of Population is conducted every five years and provides a comprehensive snapshot of the demographic characteristics of the Canadian population.\n\n5. **Health Surveys**: These surveys collect data on the health status, lifestyle, and use of health services by Canadians. An example is the Canadian Community Health Survey.\n\n6. **Environmental Surveys**: These surveys gather information on environmental factors, such as pollution, water use, and energy consumption. An example is the Households and the Environment Survey.\n\nThese surveys help inform public policy, support economic planning, and contribute to an understanding of Canadian society and its various dynamics.\n\n\nBy using alternating user and assistant messages, we can capture the previous state of conversation in one request to the model. To manually share context across generated responses, you include the model’s previous response output as input and append that input to your next request.\nIn the following example, we ask the model about Canadian demographics, followed by a request for provincial data. Appending previous responses to new requests in this way helps ensure conversations feel natural and retain the context of previous interactions.\n\n\nShow code\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nhistory = [\n    {\n        \"role\": \"user\",\n        \"content\": \"What is the Canadian Census and how often does it occur?\"\n    }\n]\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=history,\n)\n\nprint(response.choices[0].message.content)\n\nhistory.append(response.choices[0].message)\nhistory.append({ \"role\": \"user\", \"content\": \"What's the difference between the short and long form census?. Be concise and explain it be in less than 50 words\" })\n\nsecond_response = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=history,\n)\n\nprint(second_response.choices[0].message.content)\nprint(second_response.choices[0].message.content)\n\n\nThe Canadian Census is a national program conducted by Statistics Canada to collect comprehensive data on the country's population and housing. This data is crucial for government planning, policy-making, and the allocation of resources and services. The census gathers information on a variety of demographic, social, economic, and cultural aspects, such as age, sex, marital status, household relationships, language, employment, income, and housing conditions.\n\nThe Canadian Census occurs every five years, in years ending in “1” and “6.” The most recent census took place in 2021. Every ten years, in the census years ending in \"1\", a more detailed census known as the \"long-form census\" is distributed to a sample of the population. This long-form census collects additional information that provides insights into societal trends and changes. Participation in the census is mandatory, ensuring a high response rate and accuracy of the data collected.\nThe short-form census collects basic demographic data from all households, while the long-form census, sent to a sample, gathers detailed socio-economic information such as education, employment, and language, providing deeper insights into Canada's population and living conditions.\nThe short-form census collects basic demographic data from all households, while the long-form census, sent to a sample, gathers detailed socio-economic information such as education, employment, and language, providing deeper insights into Canada's population and living conditions.\n\n\n\n\nIncluding relevant context information\nIts often useful to include additional context information the model can use to generate a response within the prompt you give to the model. There are few common reasons to do this: - to give the model access to any data data outside the data set the model was trained on. - to constrain the model’s response to a specific set of resources that you have determined will be most beneficial\nThis technique of adding additional relevant context to the model generation request is sometimes called retrieval augmented generation (RAG). You can add additional context to the prompt in many different ways, from querying from a vector database and including the text back into the prompt, or by APIs provide a built-in file search tool to generate content based on uploaded documents.\n\n\nManaging the context window\nModels can only handle so much data within the context they consider during a generation request. (Mentioned needle in haystack metric). This memory limit is called a context window, which is defined in terms of tokens (chunks of data you pass in, from text, images or audio)\nUnderstanding the context window allows us to successfully create threaded conversations and manage state across model interactions.\nThe context window is the maximum number of tokens that be used in a single request. The max tokens include input, output and reasoning token.\nAs the inputs become more complex or we include more turns in a conversation, we will need to consider both output token and context window limits. Model input and outputs are metered in tokens, which at high level parsed from inputs to analyze their content and intent and assembled to render logical outputs.\n\nOutput tokens are tokens generated by a model in response to a prompt. Each model has different limits for output tokens.\nA context window describes the total tokens that can be used for both input and output tokens (and for some models reasoning tokens)\n\nIf we create a very large prompt, often by including extra context, data, or examples of the model, we can run the risk of exceeding the allocated context window for a model, and this can result in truncated outputs."
  },
  {
    "objectID": "posts/introduction-to-llms/index.html#core-idea",
    "href": "posts/introduction-to-llms/index.html#core-idea",
    "title": "Introduction to LLMs",
    "section": "Core Idea",
    "text": "Core Idea\nThe core idea is that we can configure our LLM API for structured output instead of unstructured text. Period. This allows precise extraction and standardization of information for further processing. Period. For example, we can use the structured output to extract information from our resumes or standardize them to build a structured database. Let’s go over a couple of examples.\nNow, there are a couple of ways to get structured outputs out of a model. The first one that was introduced when structured outputs became a thing back 2023 was when I first discovered them. Structured outputs were a feature that ensured that a model would always generate responses that adhere to a JSON schema. So at that time, we had to give a JSON schema if you wanted to get structured outputs. However now, and more recently, APIs like for example, the OpenAI API just has structured outputs as a built-in feature. And in addition to supporting just JSON schema in the REST API, the OpenAI SDKs for Python and JavaScript also make it easy to define object schema using Pydantic and Zod if you’re using TypeScript respectively. So let me give an example of how you can extract information from unstructured output that conforms to a schema defined in code.\n\n\nGenerating enum values\nBefore we explore how enums work with structured outputs, let’s clarify what enums are in programming languages, particularly Python.\nAn enum (short for enumeration) is a way to create a set of named constants that represent a fixed collection of related values. Think of it as defining a specific list of allowed options that a variable can take.\nfrom enum import Enum\n\nclass Status(Enum):\n    PENDING = \"pending\"\n    APPROVED = \"approved\" \n    REJECTED = \"rejected\"\n    IN_REVIEW = \"in_review\"\n\nIn this example, Status can only be one of these four predefined values. You can’t accidentally set it to “aproved” (misspelled) or “maybe” – it must be one of the defined options.\nEnums become incredibly valuable when working with LLMs in real-world applications because they solve fundamental problems with unconstrained text generation.\nWhen LLMs generate free-form text responses, they operate in an infinite possibility space. Even for simple categorical decisions, models might express the same concept through countless linguistic variations, synonyms, or formatting differences. This variability creates a mismatch between the deterministic requirements of software systems and the inherently creative nature of language models.\nEnums transform open-ended generation into constrained selection. Instead of choosing from infinite possibilities, the model must map its understanding to one of your predefined categories.\nBy forcing the model to use your exact terminology, enums ensure that conceptually identical responses are represented identically in your system. The model’s internal understanding gets translated into your application’s vocabulary.\nDeterministic Interfaces: Enums create predictable contracts between your LLM and downstream systems. Rather than parsing and interpreting varied text outputs, your application receives known constants that can be handled with simple conditional logic.\nData Integrity at Scale: In production environments, consistency compounds over time. Enums prevent the gradual degradation of data quality that occurs when small variations accumulate across thousands or millions of LLM interactions."
  },
  {
    "objectID": "posts/introduction-to-llms/index.html#how-function-calling-works",
    "href": "posts/introduction-to-llms/index.html#how-function-calling-works",
    "title": "Introduction to LLMs",
    "section": "How Function Calling Works",
    "text": "How Function Calling Works\nSo, as we will see, function calling just simply builds on this idea of structured outputs, where function calling involves a structured interaction between your application, the model, and external functions. Here’s a breakdown of the steps:\nStep 1: Define Function Declaration\nWe define a function and its declaration within our application code that allows the user to set light And make API requests So this function could hypothetically call external services or APIs for instance\n\nDefine function declarations. So we define the function declaration in our application code. Function declarations describe the function’s name, parameters, and purpose to the model.\nCall the LLM with function declarations. Send user prompt along with the function declarations to the model. It analyzes the request and determines if a function call would be helpful. If so, it responds with a structured JSON object.\nExecute function code, bracket R responsibility, bracket close. The model does not execute the function itself. Is your application’s responsibility to process the response back and check for function call?\n\nIf yes, then extract the name and the arguments of the function and execute the corresponding function in your application.\nif No, the model has provided a direct text response to the prompt.\n\nIf a function was executed, we capture the result and send it back to the model in a subsequent turn of conversation. And it will use the result to generate a response that incorporates the information from the function call.\n\nNow note that this process can be repeated over multiple turns. So this can allow for complex interactions and workflows. It’s possible that even after providing the function call output, the model decides that it needs to use another function. And then it continues on using the calling function until it’s satisfied. And the model can also support calling multiple functions in a single turn, which is called parallel function calling. And in sequence, which is called compositional function calling."
  },
  {
    "objectID": "posts/lessons-learned-for-retrieval-augmented-generation/index.html",
    "href": "posts/lessons-learned-for-retrieval-augmented-generation/index.html",
    "title": "Lessons Learned for Retrieval Augmented Generation",
    "section": "",
    "text": "Lessons Learned for Retrieval Augmented Generation\n\nStart with a clear definition Retrieval augmented generation, or RAG, means your model writes answers with help from documents that you fetch at runtime. The model is a large language model, or LLM. Every choice you make about how you prepare, store, and fetch those documents will shape answer quality.\nMake data faithful to the original Keep tables, headings, lists, and links as they appear in the source. If your loader flattens structure, the model will miss important details. Treat data cleaning and extraction as part of the model, not as an afterthought.\nUse sources and loaders that keep structure and permissions Prefer formats that preserve structure, such as Hypertext Markup Language, often called HTML, or other structured exports. Capture who can see what, and store that as metadata so search can filter out restricted text before the model reads it.\nBuild a custom parser when generic tools fail If off the shelf tools break tables or lists, write a parser that walks the document tree and keeps rows, columns, and headers in place. This one time effort pays off with cleaner chunks and better search.\nEnrich documents during ingestion Use an LLM to fix messy extracts. Rebuild tables as clean markdown tables, standardize headings, and add short summaries. Better input leads to better search later.\nChunk by meaning, not by character count Keep logical sections together, such as a table with its title or a section with its heading. Add a two line summary and a few keywords to each chunk so search can match precise questions.\nDesign metadata for search, not only for source tracking Store titles and links, and also store short summaries, keywords, and a small set of likely questions for each chunk. Make these fields searchable and also use them to guide later steps.\nKeep enrichment artifacts in a simple store Save lists of documents, summaries, keywords, and any question sets in a store that you can reuse across jobs. This lets you improve search and prompts without reprocessing the whole collection.\nImprove the question before you search Add a small step that clarifies vague questions and splits multi part questions into simple ones. Use your saved summaries and titles to pick a short list of likely documents before you run full search.\nUse hybrid search by default Combine semantic search, which uses embeddings to find meaning, with keyword search, which matches exact words and names. Embeddings are numerical vectors that encode meaning. Merging both results helps you find more right passages and fewer wrong ones.\nClean and order the retrieved context Remove duplicates and sort the passages the way they appear in the source. The model will read a small and coherent slice of the document, which reduces mistakes.\nGive the model a complete and clear prompt Include the original question, any refined sub questions, and the cleaned context. Add simple rules, such as cite the source for each claim and do not answer outside the given context.\nAutomate evaluation to speed up learning Keep a test set of real questions with strong reference answers. Use a model as a judge to score batches on a simple scale and to explain why. This turns slow expert review into fast feedback.\nTrack helpful answers and harmful advice Measure how often answers are acceptable and how often advice is wrong or unsafe. These outcome measures reflect what users care about more than exact text overlap.\nEnforce permissions inside search and generation Apply access rules before retrieval and before generation. If the user cannot see a passage, the system should not fetch it and the model should not read it.\nPlan for growth and reflection Support iterative retrieval where the model can ask for more context after the first pass. Add a self critique step that checks claims against the provided passages. Expose key actions, such as search and summarize, as tools that the model can call when needed. Extend to images later with proper extractors.\nInvest first in data and search, then in larger models The biggest gains come from faithful extraction, smart chunking, rich metadata, careful question shaping, and hybrid search. Larger models help, but they cannot invent context that was never captured or retrieved.\nCreate a loop that rewards good documentation Show teams that clear and well structured documents lead to better answers. This encourages them to improve the source content, which then further improves the system."
  },
  {
    "objectID": "principles.html",
    "href": "principles.html",
    "title": "My Guiding Principles",
    "section": "",
    "text": "Work hard. Extreme people get extreme results. Hard work compounds like interest, the earlier you do it, the more time you have for the benefits to pay off.\nHumans have been writing down their best ideas for 5,000 years. Read them. Make friends with smart dead people. Seneca, Feynman, Montaigne, Charlie Munger—whoever interests you. Read their writing. Steal their ideas. They don’t need them anymore.\nAlways have the courage to be disliked for your true self rather than being liked for someone you are not.\nBurnout ≠ Working Too Much. Burnout stems from work that doesn’t align with your true values. It’s the result of meaningless work.\nSet ambitious goals but focus all your energy on actions you can control. Worrying about the uncontrollables is almost always unproductive.\nDon’t divide your attention. Focusing on one thing yields increasing returns for each unit of effort.\nLearning should transform how you act. If an idea doesn’t influence your actions, what was the purpose of learning it in the first place?\nAlways engage with and understand the very best version of your opponent’s argument.\nTake the time to listen to smart people with whom you disagree.\nDo the unpleasant task first. Don’t multitask. Always work harder than you think you should. Hard work is just a habit.\nFocus on what is present in front of you, and pay full attention to it.\nDon’t fall into the trap of endless dissatisfaction. You can be highly ambitious and still appreciate the present.\nStay detached from things that are not up to you.\n\n\n“Not being able to govern events, I govern myself, and if they will not adapt to me, I adapt to them.”\n—Montaigne, Of Presumption (1580)\n\n\n“Human life the Stoics appear to have considered as a game of great skill; in which, however, there was a mixture of chance, or of what is vulgarly understood to be chance…. If we placed our happiness in winning the stake, we placed it in what depended upon causes beyond our power, and out of our direction. We necessarily exposed ourselves to perpetual fear and uneasiness, and frequently to grievous and mortifying disappointments. If we placed it in playing well, in playing fairly, in playing wisely and skillfully; in the propriety of our own conduct in short; we placed it in what, by proper discipline, education, and attention, might be altogether in our own power, and under our own direction. Our happiness was perfectly secure, and beyond the reach of fortune.”\n—Adam Smith, The Theory of Moral Sentiments (1759)\n\n\nDo as much as you can every day; figure out what your primary focus is and make progress on that every day.\n“Hard work” is not the same as “valuable work.”\nPeople systematically undervalue their time. Try to overvalue your time.\nAlways be high integrity, even when it costs you. The shortcuts aren’t worth it.\nSurround yourself with truly excellent people. Top 1% in some important way.\nWe don’t react to events; we react to our judgments about them, and the judgments are up to us. There is the objective reality of what happens, and there is the subjective reality of how what happened is seen, interpreted, made meaningful. These three quotes span 1600 years and basically echo this underlying idea:\n\n\n“If any external thing causes you distress, it is not the thing itself that troubles you, but your own judgment about it. And this you have the power to eliminate now.”\n—Marcus Aurelius, Meditations (161 AD)\n\n\n“It is not what things are objectively and in themselves, but what they are for us, in our way of looking at them, that makes us happy or the reverse.”\n—Schopenhauer, The Wisdom of Life (1851)\n\n\n“There is nothing either good or bad, but thinking makes it so.”\n—Shakespeare, Hamlet (1603)\n\n\nMake deliberate decisions. Where to live, what kind of people to surround yourself with, what habits to cultivate. These choices will define you. Living “authentically” means being honest with yourself. “Am I happy with the direction I’m going? Does it align with my values?” If not, remember you have the agency to change course. If your definition of a good life differs from societal or family expectations, have the courage to pursue what resonates with you. Own your life."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "My Writing",
    "section": "",
    "text": "Introduction to LLMs\n\n\n\n\n\nAn introduction to Large Language Models and their applications\n\n\n\n\n\nMay 29, 2025\n\n\nHanan Ather\n\n\n\n\n\n\n\n\n\n\n\n\nLessons Learned for Retrieval Augmented Generation\n\n\n\n\n\nEighteen practical guidelines for building robust RAG systems.\n\n\n\n\n\nAug 31, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hanan Ather",
    "section": "",
    "text": "I am a Member of Technical Staff at the Centre for AI Research and Excellence (CAIRE) at Statistics Canada. My research has focused on uncertainty quantification for LLMs (calibration + conformal prediction) and AI security & safety for confidential-data settings (data privacy, threat modeling, and evaluation)."
  },
  {
    "objectID": "index.html#graduate-course-notes",
    "href": "index.html#graduate-course-notes",
    "title": "Hanan Ather",
    "section": "Graduate Course Notes",
    "text": "Graduate Course Notes\nProbability Theory II\nReal Analysis\nStatistical Machine Learning\nGraduate Probability Theory\nMathematical Statistics II\nMathematical Statistics I"
  },
  {
    "objectID": "index.html#papers-publications",
    "href": "index.html#papers-publications",
    "title": "Hanan Ather",
    "section": "Papers & Publications",
    "text": "Papers & Publications\nMaster Thesis: Deep Reinforcement Learning\nLaw of the Iterated Logarithm\nMAT4376X Research Project"
  },
  {
    "objectID": "AGENTS.html",
    "href": "AGENTS.html",
    "title": "Repository Guidelines",
    "section": "",
    "text": "index.qmd: Homepage content.\nposts/: Blog posts. Each post lives in its own folder as posts/&lt;slug&gt;/index.qmd or index.ipynb with any images stored alongside.\nprinciples.qmd: About/principles page.\n_quarto.yml: Primary Quarto site config (outputs to docs/).\ndocs/: Generated site for GitHub Pages. Do not edit by hand.\nwebsite_docs/: Static PDFs and assets linked from pages.\n\n\n\n\n\nquarto preview: Run local dev server with live reload.\nquarto render: Build the entire site into docs/.\nquarto render posts/&lt;slug&gt;/index.ipynb: Build a single post.\nquarto clean: Clear caches (useful if freeze causes stale outputs).\nquarto check: Verify Quarto installation and environment.\n\n\n\n\n\nContent: Prefer Quarto Markdown (.qmd) or notebooks (.ipynb).\nFrontmatter: YAML with 2‑space indentation; include title, date, description, and relevant categories.\nSlugs: Lowercase kebab‑case for post folders (e.g., introduction-to-llms).\nMedia: Place images next to the post using relative paths; include alt text.\nFormatting: Use fenced code blocks with language tags (e.g., ```python).\n\n\n\n\n\nEnsure quarto render completes without errors before pushing.\nFor notebooks, run all cells top‑to‑bottom and commit outputs only when intentional; rely on execute.freeze: auto to cache heavy runs.\nValidate internal links and images in quarto preview.\n\n\n\n\n\nCommits: Use clear, present‑tense subjects (e.g., “Update navbar links”), keep to one logical change per commit. Conventional Commits are welcome but not required.\nPRs: Provide a concise description, link related issues, and include a screenshot or local URL from quarto preview for visual changes.\nDo not edit docs/ directly. It is generated by quarto render. If deployment relies on committed output, let maintainers handle publishing.\n\n\n\n\n\nNever commit secrets; this is a static site. Review _quarto.yml for nav/theme changes.\nGitHub Pages serves from docs/ on the default branch; ensure builds target that directory."
  },
  {
    "objectID": "AGENTS.html#project-structure-module-organization",
    "href": "AGENTS.html#project-structure-module-organization",
    "title": "Repository Guidelines",
    "section": "",
    "text": "index.qmd: Homepage content.\nposts/: Blog posts. Each post lives in its own folder as posts/&lt;slug&gt;/index.qmd or index.ipynb with any images stored alongside.\nprinciples.qmd: About/principles page.\n_quarto.yml: Primary Quarto site config (outputs to docs/).\ndocs/: Generated site for GitHub Pages. Do not edit by hand.\nwebsite_docs/: Static PDFs and assets linked from pages."
  },
  {
    "objectID": "AGENTS.html#build-test-and-development-commands",
    "href": "AGENTS.html#build-test-and-development-commands",
    "title": "Repository Guidelines",
    "section": "",
    "text": "quarto preview: Run local dev server with live reload.\nquarto render: Build the entire site into docs/.\nquarto render posts/&lt;slug&gt;/index.ipynb: Build a single post.\nquarto clean: Clear caches (useful if freeze causes stale outputs).\nquarto check: Verify Quarto installation and environment."
  },
  {
    "objectID": "AGENTS.html#coding-style-naming-conventions",
    "href": "AGENTS.html#coding-style-naming-conventions",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Content: Prefer Quarto Markdown (.qmd) or notebooks (.ipynb).\nFrontmatter: YAML with 2‑space indentation; include title, date, description, and relevant categories.\nSlugs: Lowercase kebab‑case for post folders (e.g., introduction-to-llms).\nMedia: Place images next to the post using relative paths; include alt text.\nFormatting: Use fenced code blocks with language tags (e.g., ```python)."
  },
  {
    "objectID": "AGENTS.html#testing-guidelines",
    "href": "AGENTS.html#testing-guidelines",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Ensure quarto render completes without errors before pushing.\nFor notebooks, run all cells top‑to‑bottom and commit outputs only when intentional; rely on execute.freeze: auto to cache heavy runs.\nValidate internal links and images in quarto preview."
  },
  {
    "objectID": "AGENTS.html#commit-pull-request-guidelines",
    "href": "AGENTS.html#commit-pull-request-guidelines",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Commits: Use clear, present‑tense subjects (e.g., “Update navbar links”), keep to one logical change per commit. Conventional Commits are welcome but not required.\nPRs: Provide a concise description, link related issues, and include a screenshot or local URL from quarto preview for visual changes.\nDo not edit docs/ directly. It is generated by quarto render. If deployment relies on committed output, let maintainers handle publishing."
  },
  {
    "objectID": "AGENTS.html#security-configuration-tips",
    "href": "AGENTS.html#security-configuration-tips",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Never commit secrets; this is a static site. Review _quarto.yml for nav/theme changes.\nGitHub Pages serves from docs/ on the default branch; ensure builds target that directory."
  }
]