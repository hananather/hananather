[
  {
    "objectID": "CLAUDE.html",
    "href": "CLAUDE.html",
    "title": "CLAUDE.md",
    "section": "",
    "text": "This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n\nThis is a Quarto-based personal blog and portfolio website for Hanan Ather. The site uses Quarto to generate static HTML from Markdown and Jupyter notebook sources.\n\n\n\n# Preview the site locally with live reload\nquarto preview\n\n# Build the site (outputs to docs/ directory)\nquarto render\n\n# Build a specific file\nquarto render index.qmd\nquarto render posts/introduction-to-llms/index.ipynb\n\n\n\n\n**_quarto.yml**: Main configuration file for the Quarto website\nindex.qmd: Homepage with bio, CV link, and course notes\nposts/: Blog posts directory (supports .qmd and .ipynb files)\n\nindex.qmd: Blog listing page\n\nprinciples.qmd: About/principles page\ndocs/: Generated static site output (do not edit directly)\nwebsite_docs/: PDF documents and static assets\n\n\n\n\nThe site is configured as a Quarto website project that: - Outputs to docs/ directory for GitHub Pages hosting - Uses Cosmo (light) and Cyborg (dark) themes - Has a simple navbar with Home, My Writing, and About sections - Uses freeze: auto for computational documents to cache outputs\n\n\n\n\nAdding a new blog post:\n\nCreate a new directory under posts/ with your post name\nAdd either an index.qmd (Markdown) or index.ipynb (Jupyter notebook)\nInclude YAML frontmatter with title, date, author, description, and categories\nRun quarto render to build\n\nModifying site structure:\n\nEdit _quarto.yml for navigation, themes, or global settings\nIndividual page settings can be overridden in their YAML frontmatter\n\nWorking with Jupyter notebooks:\n\nNotebooks in posts/ are automatically converted to HTML\nUse freeze: auto to cache computational outputs\nInclude images as attachments or separate files\n\n\n\n\n\nThe repository uses GitHub Pages deployed from the docs/ folder on the main branch. Always render the site before committing changes that affect content.\n\n\n\n\nThe docs/ directory is the build output - never edit these files directly\nAll content changes should be made to source files (.qmd, .ipynb)\nThe site uses responsive Bootstrap themes with automatic light/dark mode switching\nPDF documents in website_docs/ are directly linked from the homepage"
  },
  {
    "objectID": "CLAUDE.html#project-overview",
    "href": "CLAUDE.html#project-overview",
    "title": "CLAUDE.md",
    "section": "",
    "text": "This is a Quarto-based personal blog and portfolio website for Hanan Ather. The site uses Quarto to generate static HTML from Markdown and Jupyter notebook sources."
  },
  {
    "objectID": "CLAUDE.html#build-commands",
    "href": "CLAUDE.html#build-commands",
    "title": "CLAUDE.md",
    "section": "",
    "text": "# Preview the site locally with live reload\nquarto preview\n\n# Build the site (outputs to docs/ directory)\nquarto render\n\n# Build a specific file\nquarto render index.qmd\nquarto render posts/introduction-to-llms/index.ipynb"
  },
  {
    "objectID": "CLAUDE.html#project-structure",
    "href": "CLAUDE.html#project-structure",
    "title": "CLAUDE.md",
    "section": "",
    "text": "**_quarto.yml**: Main configuration file for the Quarto website\nindex.qmd: Homepage with bio, CV link, and course notes\nposts/: Blog posts directory (supports .qmd and .ipynb files)\n\nindex.qmd: Blog listing page\n\nprinciples.qmd: About/principles page\ndocs/: Generated static site output (do not edit directly)\nwebsite_docs/: PDF documents and static assets"
  },
  {
    "objectID": "CLAUDE.html#key-configuration",
    "href": "CLAUDE.html#key-configuration",
    "title": "CLAUDE.md",
    "section": "",
    "text": "The site is configured as a Quarto website project that: - Outputs to docs/ directory for GitHub Pages hosting - Uses Cosmo (light) and Cyborg (dark) themes - Has a simple navbar with Home, My Writing, and About sections - Uses freeze: auto for computational documents to cache outputs"
  },
  {
    "objectID": "CLAUDE.html#development-workflow",
    "href": "CLAUDE.html#development-workflow",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Adding a new blog post:\n\nCreate a new directory under posts/ with your post name\nAdd either an index.qmd (Markdown) or index.ipynb (Jupyter notebook)\nInclude YAML frontmatter with title, date, author, description, and categories\nRun quarto render to build\n\nModifying site structure:\n\nEdit _quarto.yml for navigation, themes, or global settings\nIndividual page settings can be overridden in their YAML frontmatter\n\nWorking with Jupyter notebooks:\n\nNotebooks in posts/ are automatically converted to HTML\nUse freeze: auto to cache computational outputs\nInclude images as attachments or separate files"
  },
  {
    "objectID": "CLAUDE.html#git-workflow",
    "href": "CLAUDE.html#git-workflow",
    "title": "CLAUDE.md",
    "section": "",
    "text": "The repository uses GitHub Pages deployed from the docs/ folder on the main branch. Always render the site before committing changes that affect content."
  },
  {
    "objectID": "CLAUDE.html#important-notes",
    "href": "CLAUDE.html#important-notes",
    "title": "CLAUDE.md",
    "section": "",
    "text": "The docs/ directory is the build output - never edit these files directly\nAll content changes should be made to source files (.qmd, .ipynb)\nThe site uses responsive Bootstrap themes with automatic light/dark mode switching\nPDF documents in website_docs/ are directly linked from the homepage"
  },
  {
    "objectID": "rl-labs/part-5-temporal-difference-learning/index.html",
    "href": "rl-labs/part-5-temporal-difference-learning/index.html",
    "title": "Part 5: Temporal Difference Learning",
    "section": "",
    "text": "Temporal-Difference (TD) learning is a powerful model-free approach that blends Monte Carlo and Dynamic Programming ideas. Like MC, TD learns from raw experience without a model. Like DP, TD updates estimates based partly on other learned estimates (bootstrapping). This allows TD to learn online after each step, without waiting for the episode to finish. Key concepts: - TD(0) Prediction: Also known as one-step TD or TD(0), updates the value \\(V(s)\\) toward the observed reward plus the value of the next state: \\[V(s) \\leftarrow V(s) + \\alpha [R_{t+1} + \\gamma V(s_{t+1}) - V(s)]\\] This update is based on the TD error \\(\\delta = R_{t+1} + \\gamma V(s_{t+1}) - V(s)\\).\nLet’s illustrate TD(0) prediction first, then implement SARSA and Q-learning for a control task."
  },
  {
    "objectID": "rl-labs/part-5-temporal-difference-learning/index.html#sarsa-vs-q-learning-on-grid-world",
    "href": "rl-labs/part-5-temporal-difference-learning/index.html#sarsa-vs-q-learning-on-grid-world",
    "title": "Part 5: Temporal Difference Learning",
    "section": "SARSA vs Q-Learning on Grid World",
    "text": "SARSA vs Q-Learning on Grid World\nWe’ll use the grid world, but introduce a slight twist: let’s add a “bad state” to illustrate the difference between on-policy and off-policy. A classic example is the “Cliff” environment (Sutton & Barto Example 6.6) where following the optimal path has risk of falling off a cliff (big negative reward) vs a safer path.\nFor simplicity, let’s create a small grid where: - Start at (0,0), goal at (0,3). - There’s a “cliff” at positions (1,1) for example that gives a large negative reward if stepped into. - The optimal policy might go around the cliff in an on-policy method vs jump over in off-policy.\nSARSA and Q-learning: - SARSA will update using the action it actually takes next (which, if it’s following \\(\\epsilon\\)-greedy, might be suboptimal, so it ends up learning the value of its exploratory policy). - Q-learning will update as if it always takes the optimal next action (even if it didn’t), thus it tends to learn optimistic values and converge to the true optimal policy value.\nLet’s still do a simple grid with some penalty state:\n\n\nShow code\ngrid_rows, grid_cols = 3, 4\nstart_state = (0,0)\ngoal_state = (0,3)\ncliff_state = (1,2)  # stepping here gives -10 reward (for example)\n\n\nWe will implement a step function for this:\n\n\nShow code\ndef grid_step(state, action):\n    if state == goal_state:\n        return state, 0  # absorbing\n    r, c = state\n    dr, dc = action\n    nr, nc = r+dr, c+dc\n    # bounds check\n    if nr &lt; 0 or nr &gt;= grid_rows or nc &lt; 0 or nc &gt;= grid_cols:\n        nr, nc = r, c  # hit wall, stay (could also give a small neg reward)\n    new_state = (nr, nc)\n    # reward logic\n    if new_state == cliff_state:\n        reward = -10\n    elif new_state == goal_state:\n        reward = +10\n    else:\n        reward = -1  # each step cost\n    return new_state, reward\n\n\nNow define actions and run SARSA and Q-learning episodes.\nWe’ll keep it short and not do too many episodes for demonstration.\n\n\nShow code\nactions = [(0,1),(0,-1),(1,0),(-1,0)]  # right, left, down, up moves\ndef epsilon_greedy_action(Q, state, epsilon=0.1):\n    if random.random() &lt; epsilon:\n        return random.choice(actions)\n    else:\n        # choose best action from Q\n        q_values = [Q.get((state,a), 0) for a in actions]\n        max_q = max(q_values)\n        best_actions = [a for a, q in zip(actions, q_values) if q == max_q]\n        return random.choice(best_actions)\n\ndef run_sarsa(alpha=0.5, gamma=1.0, epsilon=0.1, episodes=1000):\n    Q = {}\n    for ep in range(episodes):\n        state = start_state\n        action = epsilon_greedy_action(Q, state, epsilon)\n        while state != goal_state:\n            next_state, reward = grid_step(state, action)\n            next_action = epsilon_greedy_action(Q, next_state, epsilon)\n            # Q(s,a) update\n            current_q = Q.get((state, action), 0.0)\n            next_q = Q.get((next_state, next_action), 0.0)\n            td_target = reward + gamma * next_q\n            Q[(state, action)] = current_q + alpha * (td_target - current_q)\n            state = next_state\n            action = next_action\n    return Q\n\ndef run_q_learning(alpha=0.5, gamma=1.0, epsilon=0.1, episodes=1000):\n    Q = {}\n    for ep in range(episodes):\n        state = start_state\n        while state != goal_state:\n            action = epsilon_greedy_action(Q, state, epsilon)\n            next_state, reward = grid_step(state, action)\n            # Q-learning update\n            current_q = Q.get((state, action), 0.0)\n            # next state's best action value\n            next_q_values = [Q.get((next_state,a), 0.0) for a in actions]\n            next_max = max(next_q_values) if next_q_values else 0.0\n            td_target = reward + gamma * next_max\n            Q[(state, action)] = current_q + alpha * (td_target - current_q)\n            state = next_state\n    return Q\n\n# Run SARSA and Q-learning\nQ_sarsa = run_sarsa(episodes=1000)\nQ_qlearn = run_q_learning(episodes=1000)\n\n# Derive policies from Q (greedy)\npolicy_sarsa = {}\npolicy_qlearn = {}\nfor r in range(grid_rows):\n    for c in range(grid_cols):\n        s = (r,c)\n        # skip goal\n        if s == goal_state:\n            continue\n        # find best action in Q for state\n        q_vals_sarsa = {a: Q_sarsa.get((s,a), 0.0) for a in actions}\n        q_vals_q = {a: Q_qlearn.get((s,a), 0.0) for a in actions}\n        best_a_sarsa = max(q_vals_sarsa, key=q_vals_sarsa.get)\n        best_a_q = max(q_vals_q, key=q_vals_q.get)\n        policy_sarsa[s] = best_a_sarsa\n        policy_qlearn[s] = best_a_q\n\nprint(\"Greedy policy from SARSA:\")\nfor r in range(grid_rows):\n    row = []\n    for c in range(grid_cols):\n        s = (r,c)\n        if s == goal_state:\n            row.append(\"G\")\n        elif policy_sarsa.get(s) is None:\n            row.append(\".\")\n        else:\n            arrow = { (0,1): \"&gt;\", (0,-1): \"&lt;\", (1,0): \"v\", (-1,0): \"^\" }\n            row.append(arrow[policy_sarsa[s]])\n    print(\" \".join(row))\n\nprint(\"\\nGreedy policy from Q-learning:\")\nfor r in range(grid_rows):\n    row = []\n    for c in range(grid_cols):\n        s = (r,c)\n        if s == goal_state:\n            row.append(\"G\")\n        elif policy_qlearn.get(s) is None:\n            row.append(\".\")\n        else:\n            arrow = { (0,1): \"&gt;\", (0,-1): \"&lt;\", (1,0): \"v\", (-1,0): \"^\" }\n            row.append(arrow[policy_qlearn[s]])\n    print(\" \".join(row))\n\n\nGreedy policy from SARSA:\n&lt; &gt; &gt; G\n^ ^ ^ ^\n^ ^ &lt; &lt;\n\nGreedy policy from Q-learning:\n&gt; &gt; &gt; G\n^ ^ ^ ^\nv &gt; &lt; ^\n\n\n\nOn-Policy vs. Off-Policy\nConsider the general Temporal Difference (TD) update:\n\\[\nQ(s,a) \\leftarrow Q(s,a) + \\alpha\\Bigl[\\underbrace{r + \\gamma Q(s', a')}_{\\text{on-policy (SARSA)}} - Q(s,a)\\Bigr]\n\\]\nThe critical point making this on-policy is that the value of the next state-action pair (Q(s’, a’)) comes directly from the action (a’) actually taken by the current policy, which could include exploration.\nIn contrast, the Q-Learning update is:\n\\[\nQ(s,a) \\leftarrow Q(s,a) + \\alpha\\Bigl[\\underbrace{r + \\gamma \\max_{a'}Q(s', a')}_{\\text{off-policy (Q-Learning)}} - Q(s,a)\\Bigr]\n\\]\nHere, the critical off-policy part is clearly marked by the presence of the (_{a’}Q(s’,a’)). This means the update is computed using the best possible next action, regardless of the actual exploratory action chosen by the current policy. Thus, the policy being evaluated (greedy) differs from the behavior policy used to select actions (e.g., epsilon-greedy).\nIn our above code:\n\nSARSA explicitly selects the next action (next_action) via the same policy used for exploration (epsilon-greedy), thus directly using this action’s value:\n\ntd_target = reward + gamma * Q.get((next_state, next_action), 0.0)\n\nQ-Learning does not directly select an action via exploration for updating, but instead takes the maximum value of the next state’s Q-values:\n\nnext_max = max([Q.get((next_state, a), 0.0) for a in actions])\ntd_target = reward + gamma * next_max\nThis explicitly demonstrates why SARSA is on-policy (next action is sampled from the policy being improved), while Q-learning is off-policy (next action used in the update is always the greedy, optimal action, not necessarily chosen by the exploratory policy).\nTo reiterate: - On-policy methods (SARSA) learn about the policy being executed, which includes exploration. They generally result in safer learning (they don’t propagate unrealistically high values for risky states because those high values won’t materialize under the \\(\\epsilon\\)-greedy behavior that might fall off). - Off-policy (Q-learning) learns the optimal target policy independent of the current behavior. It can converge to the optimal faster, but one must be careful with convergence conditions (need sufficient exploration, etc.).\n\n← Part 4 | All labs"
  },
  {
    "objectID": "rl-labs/part-1-prediction-and-control/index.html",
    "href": "rl-labs/part-1-prediction-and-control/index.html",
    "title": "Part 1: Prediction and Control",
    "section": "",
    "text": "Now that we’ve covered basic probability, let’s dive into a simple reinforcement learning scenario: a gambling game. This example will introduce core RL ideas of prediction (evaluating how good a given strategy is) and control (finding a better strategy). We will define a simple gambling environment and simulate an agent’s decisions within it.\n\nProblem Setup: A Gambler’s Game\nImagine a gambler who starts with a certain amount of money and aims to reach a target amount. At each turn, the gambler can bet some of their money on a coin flip: - If the coin comes up Heads (win), the gambler wins the amount bet (it gets added to their capital). - If Tails (lose), the gambler loses the amount bet (subtracted from their capital). - The coin might be biased with probability p of Heads.\nThe game ends either when the gambler reaches the target (goal) or loses all money (bust).\nThis is known as the Gambler’s Problem from Sutton and Barto’s RL textbook. - States: The gambler’s capital (from \\(0\\) to \\(Goal\\)). - Actions: The bet amount (from \\(1\\) up to the smaller of current capital or amount needed to reach goal). - Transition: Win or lose based on the coin flip. - Reward: We can define a reward of +1 for reaching the goal, and 0 otherwise (no reward each step except at the end). This way, maximizing total reward is equivalent to maximizing the probability of reaching the goal.\nFor simplicity, let’s implement a smaller version: goal = $10, starting capital = $5, and a fair coin (p = 0.5). The gambler can bet $1 each turn (to keep it simple initially).\n\nSimulation of a Fixed Policy\nPolicy: Let’s start with a simple policy: always bet $1 until the game ends.\nWe will simulate many episodes (games) with this policy to estimate: - Probability of reaching the goal (this is the value of the starting state under this policy). - The distribution of episode lengths (how many bets until finish).\nLet’s write a simulation for one episode and then loop it for many episodes.\n\n\nShow code\nimport random\n\ndef simulate_gamble(start_capital, goal, p_head=0.5):\n    \"\"\"\n    Simulate one episode of the gambling game.\n    Returns True if goal reached (win), False if gambler goes broke (lose).\n    Policy: always bet 1.\n    \"\"\"\n    capital = start_capital\n    while 0 &lt; capital &lt; goal:\n        # Always bet 1\n        bet = 1\n        if random.random() &lt; p_head:  # win\n            capital += bet\n        else:  # lose\n            capital -= bet\n        # No intermediate rewards in this formulation\n    # Loop ends when capital == 0 or capital == goal\n    return capital &gt;= goal  # True if reached goal, False if busted\n\n# Test one episode\nresult = simulate_gamble(5, 10, p_head=0.5)\nprint(\"Episode result:\", \"Reached goal!\" if result else \"Broke :(\")\n\n\nEpisode result: Broke :(\n\n\nRun the above cell a few times to see different outcomes (sometimes the gambler wins, sometimes loses).\nNow, let’s estimate the probability of winning from the starting capital of 5 with this policy by simulating many episodes:\n\n\nShow code\nimport math\n\ntrials = 10000\nwins = 0\nfor _ in range(trials):\n    if simulate_gamble(5, 10, p_head=0.5):\n        wins += 1\nwin_prob = wins / trials\nprint(f\"Estimated probability of reaching $10 starting from $5 (bet $1 policy) = {win_prob:.3f}\")\n\n\nEstimated probability of reaching $10 starting from $5 (bet $1 policy) = 0.498\n\n\nWith a fair coin and symmetric game, you might expect ~50% chance to reach the goal when starting at half the goal (in fact, for an unbiased random walk, the probability of hitting +5 before -5 starting at 0 is 50%). Our simulation likely shows close to 0.5.\nThis probability is essentially the value of the state “capital=5” under the current policy (always bet 1). In prediction terms, we’ve estimated \\(v_{\\pi}(5)\\) for our policy \\(\\pi\\) (always bet 1) as ~0.5.\nWe can similarly estimate \\(v_{\\pi}(s)\\) for other starting states s by changing start_capital.\nExercise: Try simulating from start_capital=1 and start_capital=9 to see the win probabilities. (We expect \\(v_{\\pi}(1)\\) to be much lower than \\(v_{\\pi}(9)\\), since with only \\(\\$1\\) the gambler is one loss from ruin, whereas with \\(\\$9\\) they’re one step from the goal.)\n\n\n\nPrediction: State Value Function\nFor this gambling game, the state value \\(v_{\\pi}(s)\\) is the probability of reaching the goal from state \\(s\\) under policy $(because we set reward 1 for success, 0 otherwise, so the expected return equals the success probability).\nWe can approximate the entire value function for our policy by simulation:\n\n\nShow code\ndef estimate_values(policy_func, goal, p_head=0.5, trials=10000):\n    \"\"\"\n    Estimate value of each capital state 0..goal for a given policy by simulation.\n    policy_func(state) -&gt; bet amount\n    Returns a dict of state -&gt; estimated value.\n    \"\"\"\n    values = {s: 0.0 for s in range(goal+1)}\n    for s in range(1, goal):  # exclude terminal states 0 and goal\n        wins = 0\n        for _ in range(trials):\n            # simulate from state s\n            capital = s\n            while 0 &lt; capital &lt; goal:\n                bet = policy_func(capital)\n                if random.random() &lt; p_head:\n                    capital += bet\n                else:\n                    capital -= bet\n            if capital &gt;= goal:\n                wins += 1\n        values[s] = wins / trials\n    values[0] = 0.0\n    values[goal] = 1.0\n    return values\n\n# Define our always-bet-1 policy function\npolicy_bet1 = lambda capital: 1\n\nval_estimates = estimate_values(policy_bet1, goal=10, p_head=0.5, trials=2000)\nprint(\"Estimated value of each state (always bet 1 policy):\")\nfor s in range(0, 11):\n    print(f\"v({s}) ≈ {val_estimates[s]:.2f}\")\n\n\nEstimated value of each state (always bet 1 policy):\nv(0) ≈ 0.00\nv(1) ≈ 0.10\nv(2) ≈ 0.19\nv(3) ≈ 0.31\nv(4) ≈ 0.40\nv(5) ≈ 0.50\nv(6) ≈ 0.58\nv(7) ≈ 0.71\nv(8) ≈ 0.80\nv(9) ≈ 0.91\nv(10) ≈ 1.00\n\n\nYou should see the values increasing as the capital gets higher (closer to the goal). State 0 has value 0 (game lost) and state 10 has value 1 (goal achieved). For a fair coin, the value might roughly increase linearly with the capital (since each \\(\\$1\\) is like one step closer to victory on average). In fact, for the fair coin random walk, the true \\(v_{\\pi}(s)\\) should be \\(s/10\\) in this scenario (starting with \\(s\\) dollars out of 10)\n\n\nControl: Comparing Different Policies\nNow we have a way to evaluate a given policy (by simulation). The next step is control: finding a better policy. Let’s compare two policies: - \\(\\pi_1\\): Always bet 1 (we evaluated this). - \\(\\pi_2\\): Always bet all-in (bet all current capital each time).\nIntuition: - \\(\\pi_2\\) is riskier but reaches the goal faster if it wins. For a fair coin, any bet size yields the same success probability in theory (actually for p=0.5, any strategy gives \\(s/Goal\\) chance). But for a biased coin, the optimal strategy changes. - Let’s test with a biased coin, p = 0.4 (40% chance to win each bet, so the odds are against the gambler). In that case, a high-variance strategy (betting big) might increase success chances\n\n\nShow code\ndef simulate_policy(start_capital, goal, p_head, policy_func):\n    capital = start_capital\n    while 0 &lt; capital &lt; goal:\n        bet = policy_func(capital)\n        if random.random() &lt; p_head:\n            capital += bet\n        else:\n            capital -= bet\n    return capital &gt;= goal\n\ndef estimate_win_prob(start_capital, goal, p_head, policy_func, trials=5000):\n    wins = 0\n    for _ in range(trials):\n        if simulate_policy(start_capital, goal, p_head, policy_func):\n            wins += 1\n    return wins / trials\n\n# Define policy functions:\npolicy_all_in = lambda capital: capital  # bet all current capital\n\n# Estimate win probabilities for start=5, goal=10, p=0.4\nwin_prob_bet1 = estimate_win_prob(5, 10, p_head=0.4, policy_func=policy_bet1, trials=10000)\nwin_prob_allin = estimate_win_prob(5, 10, p_head=0.4, policy_func=policy_all_in, trials=10000)\nprint(f\"Policy π1 (bet 1) win probability ≈ {win_prob_bet1:.3f}\")\nprint(f\"Policy π2 (all-in) win probability ≈ {win_prob_allin:.3f}\")\n\n\nPolicy π1 (bet 1) win probability ≈ 0.116\nPolicy π2 (all-in) win probability ≈ 0.392\n\n\nIt’s likely that the all-in policy has a higher win probability when p &lt; 0.5. For example, starting with \\(\\$5\\), goal \\(10, p=0.4\\): - Bet-1 policy requires winning 5 times before losing 5 times (very low probability). - All-in policy needs one win of 5 (0.4 chance). So \\(\\pi_2\\) might show ~0.4 win prob, whereas \\(\\pi_1\\) could be much lower (the gambler will probably bust most of the time due to needing consecutive wins).\nResult Discussion: The better policy is the one with higher win probability (or higher expected reward). In this case, all-in seems better for p=0.4. If p were 0.6 (favorable odds), the opposite would be true: betting small repeatedly would yield a higher overall chance to eventually accumulate wins (because the odds are in your favor, you want many small bets to reduce variance). This simple comparison illustrates policy improvement: if we find a policy that yields higher value for the starting state, it’s a better policy. In reinforcement learning, we often iteratively improve a policy by looking at such comparisons.\nThis simple comparison illustrates policy improvement: if we find a policy that yields higher value for the starting state, it’s a better policy. In reinforcement learning, we often iteratively improve a policy by looking at such comparisons.\n\n\nToward Optimal Policy\nWhat is the optimal policy for the gambler? We won’t derive it fully here (that’s an exercise for Dynamic Programming later), but intuitively: - If the coin is unfair (p &lt; 0.5), optimal strategy is to take risky bets (high variance) to maximize the chance of a lucky streak - If the coin is in your favor (p &gt; 0.5), optimal is to bet small increments (low variance) to steadily accumulate wins - If \\(p = 0.5\\) exactly, many strategies yield the same 50% success probability, so the “optimal” is not unique.\nNext, we’ll explore a different classic RL problem: the multi-armed bandit, which focuses on the exploration-exploitation trade-off in a simpler setting (no multiple states, just choosing actions for immediate reward).\n\n← Part 0 | All labs | Part 2 →"
  },
  {
    "objectID": "rl-labs/part-2-multi-armed-bandits/index.html",
    "href": "rl-labs/part-2-multi-armed-bandits/index.html",
    "title": "Part 2: Multi-Armed Bandits",
    "section": "",
    "text": "Multi-armed bandits (MABs) are a simpler class of problems that capture the fundamental challenge of exploration vs. exploitation in reinforcement learning. The scenario is like a gambler in front of several slot machines (“one-armed bandits”): each pull of a lever gives a stochastic reward. The gambler (agent) wants to maximize reward over time by choosing which machines to play, balancing trying new machines (exploration) and sticking with the best one found so far (exploitation)."
  },
  {
    "objectID": "rl-labs/part-2-multi-armed-bandits/index.html#problem-setup",
    "href": "rl-labs/part-2-multi-armed-bandits/index.html#problem-setup",
    "title": "Part 2: Multi-Armed Bandits",
    "section": "Problem Setup",
    "text": "Problem Setup\n\nYou have K slot machines (arms). Each arm, when pulled, gives a reward drawn from a fixed distribution (unknown to you). For simplicity, let’s say each arm gives a reward of 1 with some probability (and 0 otherwise) – a Bernoulli bandit.\nYour goal is to maximize the total reward over a series of pulls (trials).\n\nKey challenge: Exploration vs. Exploitation - Exploration: Try different arms to gain information about their payoff. - Exploitation: Use the information to pick the arm with the highest known reward rate so far.\nWe will implement and compare three strategies: 1. \\(\\epsilon\\)-Greedy – with probability ε, explore (random arm), otherwise exploit (best arm) 2. Upper Confidence Bound (UCB) – select the arm with the highest upper confidence bound on reward (favoring arms with less trials so far to explore uncertainty 3. Thompson Sampling – a Bayesian approach: maintain a distribution for each arm’s success probability and sample from these to decide (naturally balancing exploration and exploitation)\nLet’s set up a bandit problem to test these. We will create a bandit with, say, 3 arms: - Arm 0: win probability 0.3 - Arm 1: win probability 0.5 - Arm 2: win probability 0.7\nThe best arm is #2 (70% chance of reward), but the agents don’t know that initially.\n\n\nShow code\nimport random\n\n# True probabilities for each arm (unknown to agent)\ntrue_probs = [0.3, 0.5, 0.7]\nK = len(true_probs)\n\n\nWe’ll simulate a sequence of 1000 pulls. At each step, the agent chooses an arm according to the strategy, and gets a reward 1 (with the arm’s true probability) or 0. We will track: - The cumulative reward over time. - The number of pulls of each arm (to see exploration).\n\nε-Greedy Strategy\nThe ε-greedy strategy is simple and widely used: - We maintain an estimate of each arm’s value (e.g., average reward observed). - On each step, with probability ε choose a random arm (explore), otherwise choose the arm with highest estimated value (exploit). - After getting the reward, update the arm’s value estimate (incremental average).\nLet’s implement ε-greedy:\n\n\nShow code\ndef run_epsilon_greedy(true_probs, epsilon=0.1, trials=1000):\n    # Initialize estimates and counts\n    counts = [0] * K # number of times each arm is sampled\n    values = [0.0] * K  # estimated value (mean reward) for each arm\n    rewards_history = []\n    total_reward = 0\n\n    for t in range(1, trials+1):\n        # Decide arm\n        if random.random() &lt; epsilon:\n            arm = random.randrange(K)  # explore\n        else:\n            # exploit (argmax of values, tie-break randomly)\n            max_val = max(values)\n            best_arms = [i for i, v in enumerate(values) if v == max_val]\n            arm = random.choice(best_arms)\n        # Pull arm\n        reward = 1 if random.random() &lt; true_probs[arm] else 0\n        # Update counts and value estimate\n        counts[arm] += 1\n        # Incremental average update for mean:\n        values[arm] += (reward - values[arm]) / counts[arm]\n        # Track total reward\n        total_reward += reward\n        rewards_history.append(total_reward)\n    return rewards_history, counts, values\n\n# Run epsilon-greedy\nhistory_eps, counts_eps, values_eps = run_epsilon_greedy(true_probs, epsilon=0.1, trials=90000)\nprint(\"Epsilon-Greedy results:\")\nprint(\"Arm counts:\", counts_eps)\nprint(\"Estimated values:\", [f\"{v:.2f}\" for v in values_eps])\nprint(\"Total reward:\", history_eps[-1])\n\n\nEpsilon-Greedy results:\nArm counts: [3024, 3220, 83756]\nEstimated values: ['0.30', '0.50', '0.70']\nTotal reward: 61325\n\n\nQuestion: What happens if ε is too high or too low? Feel free to try different ε values in run_epsilon_greedy to see the effect on total reward.\n\n\nUpper Confidence Bound (UCB) Strategy\nUCB is a more advanced strategy that addresses a limitation of ε-greedy: ε-greedy explores blindly. UCB uses a confidence interval approach: it picks the arm with the highest upper confidence bound on the estimated reward.One common formula (UCB1) for each arm a at time t is: \\[\n\\text{UCB}_a = \\hat{Q}_a + \\sqrt{\\frac{2 \\ln t}{N_a}}\n\\] where: - \\(\\hat{Q}_a\\) is the current estimated value (mean reward) of arm a. - \\(N_a\\) is how many times arm a has been pulled. - \\(N_a\\) is how many times arm a has been pulled.\nWe will implement UCB1. Note: We need to pull each arm at least once initially to get an estimate (and avoid division by zero).\n\n\nShow code\nimport math\n\ndef run_ucb(trials=1000):\n    counts = [0] * K\n    values = [0.0] * K \n    rewards_history = []\n    total_reward = 0\n\n    # Initial phase: play each arm once\n    for arm in range(K):\n        reward = 1 if random.random() &lt; true_probs[arm] else 0\n        counts[arm] += 1\n        values[arm] += reward  # single sample mean is the reward itself\n        total_reward += reward\n        rewards_history.append(total_reward)\n    # Now do the remaining trials\n    for t in range(K+1, trials+1):\n        # Compute UCB for each arm\n        ucb_values = []\n        for arm in range(K):\n            # Exploitation term = values[arm]\n            # Exploration term = sqrt(2 * ln(t) / counts[arm])\n            exploration_bonus = math.sqrt(2 * math.log(t) / counts[arm])\n            ucb_values.append(values[arm] + exploration_bonus)\n        # Select arm with highest UCB\n        arm = ucb_values.index(max(ucb_values))\n        # Pull arm\n        reward = 1 if random.random() &lt; true_probs[arm] else 0\n        counts[arm] += 1\n        # Update mean estimate\n        values[arm] += (reward - values[arm]) / counts[arm] # incremental mean update\n        total_reward += reward\n        rewards_history.append(total_reward)\n    return rewards_history, counts, values\n\nhistory_ucb, counts_ucb, values_ucb = run_ucb(trials=1000)\nprint(\"\\nUCB results:\")\nprint(\"Arm counts:\", counts_ucb)\nprint(\"Estimated values:\", [f\"{v:.2f}\" for v in values_ucb])\nprint(\"Total reward:\", history_ucb[-1])\n\n\n\nUCB results:\nArm counts: [71, 136, 793]\nEstimated values: ['0.37', '0.49', '0.68']\nTotal reward: 634\n\n\nThe formula values[arm] += (reward - values[arm]) / counts[arm] keeps a running average in the UCB algorithm. If you have an old average m over n samples, and you add a new value x, the new average is (m * n + x) / (n + 1). This pattern pops up elsewhere—reinforcement learning, online learning, even basic statistics."
  },
  {
    "objectID": "rl-labs/part-2-multi-armed-bandits/index.html#thompson-sampling",
    "href": "rl-labs/part-2-multi-armed-bandits/index.html#thompson-sampling",
    "title": "Part 2: Multi-Armed Bandits",
    "section": "3 Thompson Sampling",
    "text": "3 Thompson Sampling\nThompson Sampling is a Bayesian approach: - Assume a prior distribution for each arm’s success probability. For a Bernoulli reward, a common prior is Beta(1,1) (uniform). - After each pull, update the posterior for that arm (Beta prior updated with observed successes/failures). - To choose an arm: sample a success probability from each arm’s current posterior, then pick the arm with the highest sample. This way, arms with more uncertainty or higher potential can win the selection sometimes, naturally balancing exploration and exploitation.\nWe’ll implement Thompson Sampling using Beta distributions for each arm. Python’s random.betavariate(alpha, beta) can sample from a Beta distribution.\n\n\nShow code\ndef run_thompson(trials=10000):\n    # Start with prior Beta(1,1) for each arm\n    alpha = [1] * K\n    beta = [1] * K\n    counts = [0] * K\n    values = [0.0] * K  # can track mean as successes / trials for info\n    total_reward = 0\n    rewards_history = []\n\n    for t in range(1, trials+1):\n        # Sample a probability for each arm from its Beta(alpha, beta)\n        sampled_probs = [random.betavariate(alpha[a], beta[a]) for a in range(K)]\n        arm = sampled_probs.index(max(sampled_probs))\n        # Pull the chosen arm\n        reward = 1 if random.random() &lt; true_probs[arm] else 0\n        counts[arm] += 1\n        # Update alpha, beta (posterior update: alpha += reward, beta += (1-reward))\n        alpha[arm] += reward\n        beta[arm] += (1 - reward)\n        # Update value estimate for tracking\n        values[arm] += (reward - values[arm]) / counts[arm]\n        total_reward += reward\n        rewards_history.append(total_reward)\n    return rewards_history, counts, values\n\nhistory_th, counts_th, values_th = run_thompson(trials=1000)\nprint(\"\\nThompson Sampling results:\")\nprint(\"Arm counts:\", counts_th)\nprint(\"Estimated values:\", [f\"{v:.2f}\" for v in values_th])\nprint(\"Total reward:\", history_th[-1])\n\n\n\nThompson Sampling results:\nArm counts: [6, 27, 967]\nEstimated values: ['0.00', '0.41', '0.72']\nTotal reward: 704\n\n\n\n\nShow code\navg_reward_eps = history_eps[-1] / len(history_eps)\navg_reward_ucb = history_ucb[-1] / len(history_ucb)\navg_reward_th = history_th[-1] / len(history_th)\nprint(\"\\nAverage reward per pull:\")\nprint(f\"Epsilon-Greedy: {avg_reward_eps:.3f}\")\nprint(f\"UCB: {avg_reward_ucb:.3f}\")\nprint(f\"Thompson Sampling: {avg_reward_th:.3f}\")\n\n\n\nAverage reward per pull:\nEpsilon-Greedy: 0.681\nUCB: 0.634\nThompson Sampling: 0.704\n\n\n\nε-Greedy: Simple, ensures some exploration. But it doesn’t reduce exploration over time (unless ε is decayed), so it might waste some pulls exploring even when fairly confident\nUCB: Uses a principled approach to exploration. It explores less over time as it gains confidence, focusing on uncertain arms until confidence bounds narrow\nThompson Sampling: A Bayesian method that often performs as well or better by naturally balancing exploration according to uncertainty. It’s a bit more complex (requires sampling and thinking in terms of distributions), but very powerful.\n\nAll these methods are important in reinforcement learning and online decision-making. They illustrate the core exploration-exploitation trade-off in a simple setting (no state transitions, just repeated actions). In a full RL problem, similar concepts apply when an agent tries actions in an environment and must explore new actions while exploiting what it knows.Next, we will move to Dynamic Programming for RL, where we assume we know the environment dynamics and see how to compute optimal policies by planning (instead of learning from data).\n\n← Part 1 | All labs | Part 3 →"
  },
  {
    "objectID": "AGENTS.html",
    "href": "AGENTS.html",
    "title": "Repository Guidelines",
    "section": "",
    "text": "index.qmd: Homepage content.\nposts/: Blog posts. Each post lives in its own folder as posts/&lt;slug&gt;/index.qmd or index.ipynb with any images stored alongside.\nprinciples.qmd: About/principles page.\n_quarto.yml: Primary Quarto site config (outputs to docs/).\ndocs/: Generated site for GitHub Pages. Do not edit by hand.\nwebsite_docs/: Static PDFs and assets linked from pages.\n\n\n\n\n\nquarto preview: Run local dev server with live reload.\nquarto render: Build the entire site into docs/.\nquarto render posts/&lt;slug&gt;/index.ipynb: Build a single post.\nquarto clean: Clear caches (useful if freeze causes stale outputs).\nquarto check: Verify Quarto installation and environment.\n\n\n\n\n\nContent: Prefer Quarto Markdown (.qmd) or notebooks (.ipynb).\nFrontmatter: YAML with 2‑space indentation; include title, date, description, and relevant categories.\nSlugs: Lowercase kebab‑case for post folders (e.g., introduction-to-llms).\nMedia: Place images next to the post using relative paths; include alt text.\nFormatting: Use fenced code blocks with language tags (e.g., ```python).\n\n\n\n\n\nEnsure quarto render completes without errors before pushing.\nFor notebooks, run all cells top‑to‑bottom and commit outputs only when intentional; rely on execute.freeze: auto to cache heavy runs.\nValidate internal links and images in quarto preview.\n\n\n\n\n\nCommits: Use clear, present‑tense subjects (e.g., “Update navbar links”), keep to one logical change per commit. Conventional Commits are welcome but not required.\nPRs: Provide a concise description, link related issues, and include a screenshot or local URL from quarto preview for visual changes.\nDo not edit docs/ directly. It is generated by quarto render. If deployment relies on committed output, let maintainers handle publishing.\n\n\n\n\n\nNever commit secrets; this is a static site. Review _quarto.yml for nav/theme changes.\nGitHub Pages serves from docs/ on the default branch; ensure builds target that directory."
  },
  {
    "objectID": "AGENTS.html#project-structure-module-organization",
    "href": "AGENTS.html#project-structure-module-organization",
    "title": "Repository Guidelines",
    "section": "",
    "text": "index.qmd: Homepage content.\nposts/: Blog posts. Each post lives in its own folder as posts/&lt;slug&gt;/index.qmd or index.ipynb with any images stored alongside.\nprinciples.qmd: About/principles page.\n_quarto.yml: Primary Quarto site config (outputs to docs/).\ndocs/: Generated site for GitHub Pages. Do not edit by hand.\nwebsite_docs/: Static PDFs and assets linked from pages."
  },
  {
    "objectID": "AGENTS.html#build-test-and-development-commands",
    "href": "AGENTS.html#build-test-and-development-commands",
    "title": "Repository Guidelines",
    "section": "",
    "text": "quarto preview: Run local dev server with live reload.\nquarto render: Build the entire site into docs/.\nquarto render posts/&lt;slug&gt;/index.ipynb: Build a single post.\nquarto clean: Clear caches (useful if freeze causes stale outputs).\nquarto check: Verify Quarto installation and environment."
  },
  {
    "objectID": "AGENTS.html#coding-style-naming-conventions",
    "href": "AGENTS.html#coding-style-naming-conventions",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Content: Prefer Quarto Markdown (.qmd) or notebooks (.ipynb).\nFrontmatter: YAML with 2‑space indentation; include title, date, description, and relevant categories.\nSlugs: Lowercase kebab‑case for post folders (e.g., introduction-to-llms).\nMedia: Place images next to the post using relative paths; include alt text.\nFormatting: Use fenced code blocks with language tags (e.g., ```python)."
  },
  {
    "objectID": "AGENTS.html#testing-guidelines",
    "href": "AGENTS.html#testing-guidelines",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Ensure quarto render completes without errors before pushing.\nFor notebooks, run all cells top‑to‑bottom and commit outputs only when intentional; rely on execute.freeze: auto to cache heavy runs.\nValidate internal links and images in quarto preview."
  },
  {
    "objectID": "AGENTS.html#commit-pull-request-guidelines",
    "href": "AGENTS.html#commit-pull-request-guidelines",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Commits: Use clear, present‑tense subjects (e.g., “Update navbar links”), keep to one logical change per commit. Conventional Commits are welcome but not required.\nPRs: Provide a concise description, link related issues, and include a screenshot or local URL from quarto preview for visual changes.\nDo not edit docs/ directly. It is generated by quarto render. If deployment relies on committed output, let maintainers handle publishing."
  },
  {
    "objectID": "AGENTS.html#security-configuration-tips",
    "href": "AGENTS.html#security-configuration-tips",
    "title": "Repository Guidelines",
    "section": "",
    "text": "Never commit secrets; this is a static site. Review _quarto.yml for nav/theme changes.\nGitHub Pages serves from docs/ on the default branch; ensure builds target that directory."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hanan Ather",
    "section": "",
    "text": "I am a Member of Technical Staff at the Centre for AI Research and Excellence (CAIRE) at Statistics Canada. My research has focused on uncertainty quantification for LLMs (calibration + conformal prediction) and AI security & safety for confidential-data settings (data privacy, threat modeling, and evaluation). I’m a member of the Office of Responsible AI, where I lead technical reviews of AI systems across government and contribute to AI frameworks and governance guidance for international bodies including UNECE, G7 GovAI, and the International Statistical Institute (ISI).\nLinks: My CV"
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Hanan Ather",
    "section": "Projects",
    "text": "Projects\n\nI built Probability Lab, an interactive learning platform for probability, statistics, and linear regression.\nI published a 6-part series of reinforcement learning labs (Jupyter notebooks): Reinforcement Learning Labs."
  },
  {
    "objectID": "index.html#graduate-course-notes",
    "href": "index.html#graduate-course-notes",
    "title": "Hanan Ather",
    "section": "Graduate Course Notes",
    "text": "Graduate Course Notes\nProbability Theory II\nReal Analysis\nStatistical Machine Learning\nGraduate Probability Theory\nMathematical Statistics II\nMathematical Statistics I"
  },
  {
    "objectID": "index.html#papers-publications",
    "href": "index.html#papers-publications",
    "title": "Hanan Ather",
    "section": "Papers & Publications",
    "text": "Papers & Publications\nMaster Thesis: Deep Reinforcement Learning\nLaw of the Iterated Logarithm\nMAT4376X Research Project"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "My Writing",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nAug 31, 2025\n\n\nLessons Learned for Retrieval Augmented Generation\n\n\nEighteen practical guidelines for building robust RAG systems.\n\n\n\n\nMay 29, 2025\n\n\nIntroduction to LLMs\n\n\nAn introduction to Large Language Models and their applications\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "principles.html",
    "href": "principles.html",
    "title": "My Guiding Principles",
    "section": "",
    "text": "Work hard. Extreme people get extreme results. Hard work compounds like interest, the earlier you do it, the more time you have for the benefits to pay off.\nHumans have been writing down their best ideas for 5,000 years. Read them. Make friends with smart dead people. Seneca, Feynman, Montaigne, Charlie Munger—whoever interests you. Read their writing. Steal their ideas. They don’t need them anymore.\nAlways have the courage to be disliked for your true self rather than being liked for someone you are not.\nBurnout ≠ Working Too Much. Burnout stems from work that doesn’t align with your true values. It’s the result of meaningless work.\nSet ambitious goals but focus all your energy on actions you can control. Worrying about the uncontrollables is almost always unproductive.\nDon’t divide your attention. Focusing on one thing yields increasing returns for each unit of effort.\nLearning should transform how you act. If an idea doesn’t influence your actions, what was the purpose of learning it in the first place?\nAlways engage with and understand the very best version of your opponent’s argument.\nTake the time to listen to smart people with whom you disagree.\nDo the unpleasant task first. Don’t multitask. Always work harder than you think you should. Hard work is just a habit.\nFocus on what is present in front of you, and pay full attention to it.\nDon’t fall into the trap of endless dissatisfaction. You can be highly ambitious and still appreciate the present.\nStay detached from things that are not up to you.\n\n\n“Not being able to govern events, I govern myself, and if they will not adapt to me, I adapt to them.”\n—Montaigne, Of Presumption (1580)\n\n\n“Human life the Stoics appear to have considered as a game of great skill; in which, however, there was a mixture of chance, or of what is vulgarly understood to be chance…. If we placed our happiness in winning the stake, we placed it in what depended upon causes beyond our power, and out of our direction. We necessarily exposed ourselves to perpetual fear and uneasiness, and frequently to grievous and mortifying disappointments. If we placed it in playing well, in playing fairly, in playing wisely and skillfully; in the propriety of our own conduct in short; we placed it in what, by proper discipline, education, and attention, might be altogether in our own power, and under our own direction. Our happiness was perfectly secure, and beyond the reach of fortune.”\n—Adam Smith, The Theory of Moral Sentiments (1759)\n\n\nDo as much as you can every day; figure out what your primary focus is and make progress on that every day.\n“Hard work” is not the same as “valuable work.”\nPeople systematically undervalue their time. Try to overvalue your time.\nAlways be high integrity, even when it costs you. The shortcuts aren’t worth it.\nSurround yourself with truly excellent people. Top 1% in some important way.\nWe don’t react to events; we react to our judgments about them, and the judgments are up to us. There is the objective reality of what happens, and there is the subjective reality of how what happened is seen, interpreted, made meaningful. These three quotes span 1600 years and basically echo this underlying idea:\n\n\n“If any external thing causes you distress, it is not the thing itself that troubles you, but your own judgment about it. And this you have the power to eliminate now.”\n—Marcus Aurelius, Meditations (161 AD)\n\n\n“It is not what things are objectively and in themselves, but what they are for us, in our way of looking at them, that makes us happy or the reverse.”\n—Schopenhauer, The Wisdom of Life (1851)\n\n\n“There is nothing either good or bad, but thinking makes it so.”\n—Shakespeare, Hamlet (1603)\n\n\nMake deliberate decisions. Where to live, what kind of people to surround yourself with, what habits to cultivate. These choices will define you. Living “authentically” means being honest with yourself. “Am I happy with the direction I’m going? Does it align with my values?” If not, remember you have the agency to change course. If your definition of a good life differs from societal or family expectations, have the courage to pursue what resonates with you. Own your life."
  },
  {
    "objectID": "posts/lessons-learned-for-retrieval-augmented-generation/index.html",
    "href": "posts/lessons-learned-for-retrieval-augmented-generation/index.html",
    "title": "Lessons Learned for Retrieval Augmented Generation",
    "section": "",
    "text": "Lessons Learned for Retrieval Augmented Generation\n\nStart with a clear definition Retrieval augmented generation, or RAG, means your model writes answers with help from documents that you fetch at runtime. The model is a large language model, or LLM. Every choice you make about how you prepare, store, and fetch those documents will shape answer quality.\nMake data faithful to the original Keep tables, headings, lists, and links as they appear in the source. If your loader flattens structure, the model will miss important details. Treat data cleaning and extraction as part of the model, not as an afterthought.\nUse sources and loaders that keep structure and permissions Prefer formats that preserve structure, such as Hypertext Markup Language, often called HTML, or other structured exports. Capture who can see what, and store that as metadata so search can filter out restricted text before the model reads it.\nBuild a custom parser when generic tools fail If off the shelf tools break tables or lists, write a parser that walks the document tree and keeps rows, columns, and headers in place. This one time effort pays off with cleaner chunks and better search.\nEnrich documents during ingestion Use an LLM to fix messy extracts. Rebuild tables as clean markdown tables, standardize headings, and add short summaries. Better input leads to better search later.\nChunk by meaning, not by character count Keep logical sections together, such as a table with its title or a section with its heading. Add a two line summary and a few keywords to each chunk so search can match precise questions.\nDesign metadata for search, not only for source tracking Store titles and links, and also store short summaries, keywords, and a small set of likely questions for each chunk. Make these fields searchable and also use them to guide later steps.\nKeep enrichment artifacts in a simple store Save lists of documents, summaries, keywords, and any question sets in a store that you can reuse across jobs. This lets you improve search and prompts without reprocessing the whole collection.\nImprove the question before you search Add a small step that clarifies vague questions and splits multi part questions into simple ones. Use your saved summaries and titles to pick a short list of likely documents before you run full search.\nUse hybrid search by default Combine semantic search, which uses embeddings to find meaning, with keyword search, which matches exact words and names. Embeddings are numerical vectors that encode meaning. Merging both results helps you find more right passages and fewer wrong ones.\nClean and order the retrieved context Remove duplicates and sort the passages the way they appear in the source. The model will read a small and coherent slice of the document, which reduces mistakes.\nGive the model a complete and clear prompt Include the original question, any refined sub questions, and the cleaned context. Add simple rules, such as cite the source for each claim and do not answer outside the given context.\nAutomate evaluation to speed up learning Keep a test set of real questions with strong reference answers. Use a model as a judge to score batches on a simple scale and to explain why. This turns slow expert review into fast feedback.\nTrack helpful answers and harmful advice Measure how often answers are acceptable and how often advice is wrong or unsafe. These outcome measures reflect what users care about more than exact text overlap.\nEnforce permissions inside search and generation Apply access rules before retrieval and before generation. If the user cannot see a passage, the system should not fetch it and the model should not read it.\nPlan for growth and reflection Support iterative retrieval where the model can ask for more context after the first pass. Add a self critique step that checks claims against the provided passages. Expose key actions, such as search and summarize, as tools that the model can call when needed. Extend to images later with proper extractors.\nInvest first in data and search, then in larger models The biggest gains come from faithful extraction, smart chunking, rich metadata, careful question shaping, and hybrid search. Larger models help, but they cannot invent context that was never captured or retrieved.\nCreate a loop that rewards good documentation Show teams that clear and well structured documents lead to better answers. This encourages them to improve the source content, which then further improves the system."
  },
  {
    "objectID": "posts/introduction-to-llms/index.html",
    "href": "posts/introduction-to-llms/index.html",
    "title": "Introduction to LLMs",
    "section": "",
    "text": "How many of you have had a conversation like this with ChatGPT recently?\n\nUser: \"Analyze the trends in our Q3 survey data. We had 15,000 responses \nabout housing affordability across different provinces.\"\n\nChatGPT: \"Based on your description, here are some key trends I'd expect \nto see in Q3 housing affordability data:\n\n1. Regional variations likely show BC and Ontario with highest concerns\n2. Age demographics probably reveal millennials most affected\n3. Income brackets under $75K likely report significant challenges\n4. Urban vs rural differences may show urban areas more impacted\n\nTo get specific insights, I'd recommend looking at cross-tabulations \nbetween income, location, and reported affordability stress levels...\"\n\nThis highlights a fundamental challenge we face in 2025: LLMs are incredibly smart, but they’re designed for human conversation, not system integration\nIn our daily work at StatCan, we don’t just need insights we need those insights to flow into our data pipelines, our reporting systems, our analysis workflows. We need to bridge this gap between conversational AI and operational systems.\nAPI versions of these same LLMs can eliminate this translation step entirely. Instead of getting human-readable text that you then have to interpret and act upon, you can get: - Structured data that flows directly into your analysis tools - Function calls that automatically trigger the right actions in your systems - Formatted outputs that match your reporting requirements exactly\nBefore: \nAsk ChatGPT question → 2. Read response → 3. Interpret manually → 4. Write code/queries → 5. Execute actions → 6. Format results\n\n\n\nimage.png\n\n\nAfter: Send API request → 2. LLM returns structured data + triggers actions → 3. Results automatically formatted and delivered\nThe key insight is this: The same AI capabilities you use in chat can be repurposed as intelligent middleware in your data systems. Let me show you exactly what this looks like in practice, starting with a simple example that will make the concept crystal clear."
  },
  {
    "objectID": "posts/introduction-to-llms/index.html#conversation-state",
    "href": "posts/introduction-to-llms/index.html#conversation-state",
    "title": "Introduction to LLMs",
    "section": "Conversation state",
    "text": "Conversation state\nThis naturally leads us to the idea of how to manage conversation state during a model interaction. There are a few ways to manage conversation state, which is important for preserving information across multiple messages or turns in a conversation.\n\nManually manage the conversation state\nGenerally for most APIs each text generation request is independent and stateless (unless we are specific APIs like the Assistants API), but we can still implement multi-turn conversations by providing additional messages as parameters to our text generation request.\n\n\nShow code\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"What does Statistics Canada do?\"},\n        {\"role\": \"assistant\", \"content\": \"Statistics Canada is the national statistical office that collects, analyzes, and publishes statistical information about Canada's economy, society, and environment.\"},\n        {\"role\": \"user\", \"content\": \"What are their main survey types?\"},\n    ],\n)\n\nprint(response.choices[0].message.content)\n\n\nStatistics Canada conducts various types of surveys, which can be broadly categorized into the following main types:\n\n1. **Household Surveys**: These surveys gather data on the population, including labor force participation, income, health, education, and housing. Examples include the Labour Force Survey and the Census of Population.\n\n2. **Business Surveys**: These surveys collect information on the economic activities of businesses, including production, sales, and employment. Examples include the Annual Survey of Manufacturing and the Canadian Survey on Business Conditions.\n\n3. **Agricultural Surveys**: These surveys collect data on various aspects of agriculture, including crop production, livestock, and farm management practices. An example is the Census of Agriculture.\n\n4. **Census**: The Census of Population is conducted every five years and provides a comprehensive snapshot of the demographic characteristics of the Canadian population.\n\n5. **Health Surveys**: These surveys collect data on the health status, lifestyle, and use of health services by Canadians. An example is the Canadian Community Health Survey.\n\n6. **Environmental Surveys**: These surveys gather information on environmental factors, such as pollution, water use, and energy consumption. An example is the Households and the Environment Survey.\n\nThese surveys help inform public policy, support economic planning, and contribute to an understanding of Canadian society and its various dynamics.\n\n\nBy using alternating user and assistant messages, we can capture the previous state of conversation in one request to the model. To manually share context across generated responses, you include the model’s previous response output as input and append that input to your next request.\nIn the following example, we ask the model about Canadian demographics, followed by a request for provincial data. Appending previous responses to new requests in this way helps ensure conversations feel natural and retain the context of previous interactions.\n\n\nShow code\nfrom openai import OpenAI\n\nclient = OpenAI()\n\nhistory = [\n    {\n        \"role\": \"user\",\n        \"content\": \"What is the Canadian Census and how often does it occur?\"\n    }\n]\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=history,\n)\n\nprint(response.choices[0].message.content)\n\nhistory.append(response.choices[0].message)\nhistory.append({ \"role\": \"user\", \"content\": \"What's the difference between the short and long form census?. Be concise and explain it be in less than 50 words\" })\n\nsecond_response = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=history,\n)\n\nprint(second_response.choices[0].message.content)\nprint(second_response.choices[0].message.content)\n\n\nThe Canadian Census is a national program conducted by Statistics Canada to collect comprehensive data on the country's population and housing. This data is crucial for government planning, policy-making, and the allocation of resources and services. The census gathers information on a variety of demographic, social, economic, and cultural aspects, such as age, sex, marital status, household relationships, language, employment, income, and housing conditions.\n\nThe Canadian Census occurs every five years, in years ending in “1” and “6.” The most recent census took place in 2021. Every ten years, in the census years ending in \"1\", a more detailed census known as the \"long-form census\" is distributed to a sample of the population. This long-form census collects additional information that provides insights into societal trends and changes. Participation in the census is mandatory, ensuring a high response rate and accuracy of the data collected.\nThe short-form census collects basic demographic data from all households, while the long-form census, sent to a sample, gathers detailed socio-economic information such as education, employment, and language, providing deeper insights into Canada's population and living conditions.\nThe short-form census collects basic demographic data from all households, while the long-form census, sent to a sample, gathers detailed socio-economic information such as education, employment, and language, providing deeper insights into Canada's population and living conditions.\n\n\n\n\nIncluding relevant context information\nIts often useful to include additional context information the model can use to generate a response within the prompt you give to the model. There are few common reasons to do this: - to give the model access to any data data outside the data set the model was trained on. - to constrain the model’s response to a specific set of resources that you have determined will be most beneficial\nThis technique of adding additional relevant context to the model generation request is sometimes called retrieval augmented generation (RAG). You can add additional context to the prompt in many different ways, from querying from a vector database and including the text back into the prompt, or by APIs provide a built-in file search tool to generate content based on uploaded documents.\n\n\nManaging the context window\nModels can only handle so much data within the context they consider during a generation request. (Mentioned needle in haystack metric). This memory limit is called a context window, which is defined in terms of tokens (chunks of data you pass in, from text, images or audio)\nUnderstanding the context window allows us to successfully create threaded conversations and manage state across model interactions.\nThe context window is the maximum number of tokens that be used in a single request. The max tokens include input, output and reasoning token.\nAs the inputs become more complex or we include more turns in a conversation, we will need to consider both output token and context window limits. Model input and outputs are metered in tokens, which at high level parsed from inputs to analyze their content and intent and assembled to render logical outputs.\n\nOutput tokens are tokens generated by a model in response to a prompt. Each model has different limits for output tokens.\nA context window describes the total tokens that can be used for both input and output tokens (and for some models reasoning tokens)\n\nIf we create a very large prompt, often by including extra context, data, or examples of the model, we can run the risk of exceeding the allocated context window for a model, and this can result in truncated outputs."
  },
  {
    "objectID": "posts/introduction-to-llms/index.html#core-idea",
    "href": "posts/introduction-to-llms/index.html#core-idea",
    "title": "Introduction to LLMs",
    "section": "Core Idea",
    "text": "Core Idea\nThe core idea is that we can configure our LLM API for structured output instead of unstructured text. Period. This allows precise extraction and standardization of information for further processing. Period. For example, we can use the structured output to extract information from our resumes or standardize them to build a structured database. Let’s go over a couple of examples.\nNow, there are a couple of ways to get structured outputs out of a model. The first one that was introduced when structured outputs became a thing back 2023 was when I first discovered them. Structured outputs were a feature that ensured that a model would always generate responses that adhere to a JSON schema. So at that time, we had to give a JSON schema if you wanted to get structured outputs. However now, and more recently, APIs like for example, the OpenAI API just has structured outputs as a built-in feature. And in addition to supporting just JSON schema in the REST API, the OpenAI SDKs for Python and JavaScript also make it easy to define object schema using Pydantic and Zod if you’re using TypeScript respectively. So let me give an example of how you can extract information from unstructured output that conforms to a schema defined in code.\n\n\nGenerating enum values\nBefore we explore how enums work with structured outputs, let’s clarify what enums are in programming languages, particularly Python.\nAn enum (short for enumeration) is a way to create a set of named constants that represent a fixed collection of related values. Think of it as defining a specific list of allowed options that a variable can take.\nfrom enum import Enum\n\nclass Status(Enum):\n    PENDING = \"pending\"\n    APPROVED = \"approved\" \n    REJECTED = \"rejected\"\n    IN_REVIEW = \"in_review\"\n\nIn this example, Status can only be one of these four predefined values. You can’t accidentally set it to “aproved” (misspelled) or “maybe” – it must be one of the defined options.\nEnums become incredibly valuable when working with LLMs in real-world applications because they solve fundamental problems with unconstrained text generation.\nWhen LLMs generate free-form text responses, they operate in an infinite possibility space. Even for simple categorical decisions, models might express the same concept through countless linguistic variations, synonyms, or formatting differences. This variability creates a mismatch between the deterministic requirements of software systems and the inherently creative nature of language models.\nEnums transform open-ended generation into constrained selection. Instead of choosing from infinite possibilities, the model must map its understanding to one of your predefined categories.\nBy forcing the model to use your exact terminology, enums ensure that conceptually identical responses are represented identically in your system. The model’s internal understanding gets translated into your application’s vocabulary.\nDeterministic Interfaces: Enums create predictable contracts between your LLM and downstream systems. Rather than parsing and interpreting varied text outputs, your application receives known constants that can be handled with simple conditional logic.\nData Integrity at Scale: In production environments, consistency compounds over time. Enums prevent the gradual degradation of data quality that occurs when small variations accumulate across thousands or millions of LLM interactions."
  },
  {
    "objectID": "posts/introduction-to-llms/index.html#how-function-calling-works",
    "href": "posts/introduction-to-llms/index.html#how-function-calling-works",
    "title": "Introduction to LLMs",
    "section": "How Function Calling Works",
    "text": "How Function Calling Works\nSo, as we will see, function calling just simply builds on this idea of structured outputs, where function calling involves a structured interaction between your application, the model, and external functions. Here’s a breakdown of the steps:\nStep 1: Define Function Declaration\nWe define a function and its declaration within our application code that allows the user to set light And make API requests So this function could hypothetically call external services or APIs for instance\n\nDefine function declarations. So we define the function declaration in our application code. Function declarations describe the function’s name, parameters, and purpose to the model.\nCall the LLM with function declarations. Send user prompt along with the function declarations to the model. It analyzes the request and determines if a function call would be helpful. If so, it responds with a structured JSON object.\nExecute function code, bracket R responsibility, bracket close. The model does not execute the function itself. Is your application’s responsibility to process the response back and check for function call?\n\nIf yes, then extract the name and the arguments of the function and execute the corresponding function in your application.\nif No, the model has provided a direct text response to the prompt.\n\nIf a function was executed, we capture the result and send it back to the model in a subsequent turn of conversation. And it will use the result to generate a response that incorporates the information from the function call.\n\nNow note that this process can be repeated over multiple turns. So this can allow for complex interactions and workflows. It’s possible that even after providing the function call output, the model decides that it needs to use another function. And then it continues on using the calling function until it’s satisfied. And the model can also support calling multiple functions in a single turn, which is called parallel function calling. And in sequence, which is called compositional function calling."
  },
  {
    "objectID": "website_docs/My Guiding Principles 1de85548209844fbaf58dcc549713ccc.html",
    "href": "website_docs/My Guiding Principles 1de85548209844fbaf58dcc549713ccc.html",
    "title": "My Guiding Principles",
    "section": "",
    "text": "My Guiding Principles\n\nWork hard. Extreme people get extreme results. Hard work compounds like interest, the earlier you do it, the more time you have for the benefits to pay off.\nHumans have been writing down their best ideas for 5,000 years. Read them. Make friends with smart dead people. Seneca, Feynman, Montaigne, Charlie Munger —whoever interests you. Read their writing. Steal their ideas. They don’t need them anymore.\nAlways have the courage to be disliked for your true self rather than being liked for someone you are not.\nBurnout ≠ Working Too Much. Burnout stems from work that doesn’t align with your true values. Its the result of meaningless work.\nSet ambitious goals but focus all your energy on actions you can control. Worrying about the uncontrollables is almost always a unproductive.\nDon’t divide your attention. Focusing on one thing yields increasing returns for each unit of effort.\nLearning should transform how you act. If an idea doesn’t influence your actions, what was the purpose of learning it in the first place?\nAlways engage with and understand the very best version of your opponent’s argument.\nTake the time to listen to smart people with whom you disagree.\nDo the unpleasant task first. Don’t multitask. Always work harder than you think you should. Hard work is just a habit.\nFocus on what is present in front of you, and pay full attention to it.\nDon’t fall into the trap of endless dissatisfaction. You can be highly ambitious and still appreciate the present.\nStay detached from things that are not up to you.\n\n“Not being able to govern events, I govern myself, and if they will not adapt to me, I adapt to them.” -Montaigne, Of Presumption (1580)\n\n\n“Human life the Stoics appear to have considered as a game of great skill; in which, however, there was a mixture of chance, or of what is vulgarly understood to be chance…. If we placed our happiness in winning the stake, we placed it in what depended upon causes beyond our power, and out of our direction. We necessarily exposed ourselves to perpetual fear and uneasiness, and frequently to grievous and mortifying disappointments. If we placed it in playing well, in playing fairly, in playing wisely and skillfully; in the propriety of our own conduct in short; we placed it in what, by proper discipline, education, and attention, might be altogether in our own power, and under our own direction. Our happiness was perfectly secure, and beyond the reach of fortune.” -Adam Smith, The Theory of Moral Sentiments (1759)\n\nDo as much as you can every day; figure out what your primary focus is and make progress on that every day.\n“Hard work” is not the same as “valuable work.”\nPeople systematically undervalue their time. Try to overvalue your time.\nAlways be high integrity, even when it costs you. The shortcuts aren’t worth it.\nSurround yourself with truly excellent people. Top 1% in some important way.\nWe don’t react to events; we react to our judgments about them, and the judgments are up to us. There is the objective reality of what happens, and there is the subjective reality of how what happened is seen, interpreted, made meaningful. These ****three quotes span 1600 years and basically echo this underlying idea:\n\n“If any external thing causes you distress, it is not the thing itself that troubles you, but your own judgment about it. And this you have the power to eliminate now.” -Marcus Aurelius, Meditations (161 AD)\n\n\n“It is not what things are objectively and in themselves, but what they are for us, in our way of looking at them, that makes us happy or the reverse.” -Schopenhauer, The Wisdom of Life (1851)\n\n\n”There is nothing either good or bad, but thinking makes it so.”\n-Shakespear, Hamlet (1603)\n\nMake deliberate decisions. where to live, what kind of people to surround yourself with, what habits to cultivate. These choices will define you. Living “authentically” means being honest with yourself. “Am I happy with the direction I’m going? Does it align with my values?” If not, remember you have the agency to change course. If your definition of a good life differs from societal or family expectations, have the courage to pursue what resonates with you. Own your life."
  },
  {
    "objectID": "rl-labs/part-4-monte-carlo-methods/index.html",
    "href": "rl-labs/part-4-monte-carlo-methods/index.html",
    "title": "Part 4: Monte Carlo Methods",
    "section": "",
    "text": "So far, we’ve seen how to evaluate and improve policies with a known model (DP) and how to handle simple bandits. Now we tackle model-free prediction and control using Monte Carlo (MC) methods. Monte Carlo methods learn from sample episodes — they require no knowledge of environment probabilities, only the ability to sample episodes by interacting with the environment.\nMonte Carlo Prediction: Estimate the value function \\(v_{\\pi}(s)\\) by averaging returns observed after visits to state s under policy π\nFirst-Visit vs Every-Visit MC: Two ways to handle multiple visits to a state in an episode: - First-Visit MC: Only use the first time a state is visited in each episode to update its value - Every-Visit MC: Use every occurrence of the state in the episode to update the average (still converges to true value, but observations are not independent)\nMonte Carlo Control: Improve policy using MC estimates. Requires enough exploration through either: - Using Exploring Starts (start episodes in all state-action pairs eventually) or - Using \\(\\epsilon\\)-soft policies (policy that always has a non-zero chance to explore all actions)\nWe will illustrate MC prediction and control on the Gambler’s Problem (from section 2) for practice, and also demonstrate the concepts in the Grid World."
  },
  {
    "objectID": "rl-labs/part-4-monte-carlo-methods/index.html#monte-carlo-prediction-policy-evaluation",
    "href": "rl-labs/part-4-monte-carlo-methods/index.html#monte-carlo-prediction-policy-evaluation",
    "title": "Part 4: Monte Carlo Methods",
    "section": "Monte Carlo Prediction (Policy Evaluation)",
    "text": "Monte Carlo Prediction (Policy Evaluation)\nLet’s return to the gambler environment (small version: goal=10, p=0.5) and evaluate the policy “always bet 1” via Monte Carlo simulation, instead of using the known probabilities. We will implement first-visit MC prediction: 1. Simulate many episodes using the policy. 2. For each episode, record the return \\(G_t\\) from each state visited. 3. Update the value estimate for the first visit of each state by averaging the returns.\nBecause the reward is 0 until the end and 1 at goal, the return is essentially 1 if the episode ends in success, 0 if failure. So the value of a state is the probability of eventually succeeding from that state (as earlier).\n\n\nShow code\nimport random\n\n# Gambler environment from state with always-bet-1 policy\ndef generate_gambler_episode(start_capital, goal, p_head=0.5):\n    \"\"\"\n    Simulates one complete episode of the gambler's problem with a fixed bet-1 policy\n    Parameters:\n    - start_capital: Initial money the gambler has\n    - goal: Target amount to win\n    - p_head: Probability of winning each bet (default 0.5 for fair coin)\n    \"\"\"\n    episode = []\n    capital = start_capital\n    while capital not in (0, goal):\n        action = 1  # bet 1\n        next_capital = capital + 1 if random.random() &lt; p_head else capital - 1\n        reward = 0  # no intermediate reward\n        episode.append((capital, action, reward))\n        capital = next_capital\n    # Append terminal state transition\n    final_reward = 1 if capital == goal else 0\n    episode.append((capital, None, final_reward))  # terminal state\n    return episode\n\n\nThe final state is handled specially with (capital, None, final_reward) where: - None for action because it’s a terminal state - final_reward is 1 if we reached the goal, 0 if we went bust Here’s a simple example to illustrate:\n\n\nShow code\n# Example episode with start_capital=2, goal=4, p_head=0.5 might look like:\n[(2, 1, 0),    # Start with $2, bet $1, no reward yet\n (3, 1, 0),    # Won first bet (now $3), bet $1, no reward yet\n (2, 1, 0),    # Lost second bet (back to $2), bet $1, no reward yet\n (3, 1, 0),    # Won third bet (now $3), bet $1, no reward yet\n (4, None, 1)] # Won fourth bet, reached goal, get reward of 1\n\n\n[(2, 1, 0), (3, 1, 0), (2, 1, 0), (3, 1, 0), (4, None, 1)]\n\n\nThis function is crucial for Monte Carlo methods because it generates the experience samples we’ll learn from. The complete episodes allow us to calculate actual returns (G) rather than estimated values.The structure (s,a,r) matches the standard format for reinforcement learning algorithms.\n\n\nShow code\n# First-visit MC policy evaluation\ndef mc_first_visit_value(start_states, goal, p_head, policy_func, episodes=10000):\n    values = {s: 0.0 for s in range(goal)}\n    returns_sum = {s: 0.0 for s in range(goal)}\n    returns_count = {s: 0 for s in range(goal)}\n\n    for _ in range(episodes):\n        # We can start episodes from a random state (to ensure exploring starts)\n        s0 = random.choice(start_states)\n        episode = []\n        capital = s0\n        # Generate episode following policy\n        while capital not in (0, goal):\n            bet = policy_func(capital)\n            next_capital = capital + bet if random.random() &lt; p_head else capital - bet\n            reward = 0\n            episode.append((capital, reward))\n            capital = next_capital\n        # Terminal state reached\n        final_reward = 1 if capital == goal else 0\n        episode.append((capital, final_reward))\n        # Calculate returns backward\n        G = 0\n        visited = set()\n        for state, reward in reversed(episode):\n            G = reward + G  # since reward is 0 except final step (The terminal reward (0 or 1) propagates back)\n            if state not in visited and state not in (0, goal):\n                visited.add(state)\n                returns_sum[state] += G\n                returns_count[state] += 1\n                values[state] = returns_sum[state] / returns_count[state]\n    return values\n\n# Use MC to evaluate policy (always bet 1) for gambler states 1..9\npolicy_bet1_func = lambda s: 1\nmc_values = mc_first_visit_value(start_states=range(1,10), goal=10, p_head=0.5, policy_func=policy_bet1_func, episodes=5000)\nprint(\"MC estimated values for gambler states 1..9 (bet-1 policy):\")\nfor s in range(1, 10):\n    print(f\"V({s}) ≈ {mc_values[s]:.2f}\")\n\n\nMC estimated values for gambler states 1..9 (bet-1 policy):\nV(1) ≈ 0.10\nV(2) ≈ 0.19\nV(3) ≈ 0.29\nV(4) ≈ 0.39\nV(5) ≈ 0.50\nV(6) ≈ 0.60\nV(7) ≈ 0.70\nV(8) ≈ 0.81\nV(9) ≈ 0.91\n\n\nThis should output values roughly increasing linearly from state 1 to 9 (similar to earlier simulation). More episodes yield a closer approximation.\nNote: We used exploring starts by starting episodes from random states to ensure each state gets visited enough. Otherwise, if we only started at state 5 every time, states like 1 or 9 might rarely be visited under the policy.\nThe above code used first-visit MC. To switch to every-visit MC, we would remove the visited check and update every time a state appears. First-visit has the advantage that each state’s returns are i.i.d. samples of \\(v_{\\pi}(s)\\) ￼."
  },
  {
    "objectID": "rl-labs/part-4-monte-carlo-methods/index.html#monte-carlo-control-finding-optimal-policy",
    "href": "rl-labs/part-4-monte-carlo-methods/index.html#monte-carlo-control-finding-optimal-policy",
    "title": "Part 4: Monte Carlo Methods",
    "section": "Monte Carlo Control (Finding Optimal Policy)",
    "text": "Monte Carlo Control (Finding Optimal Policy)\nNow let’s try MC control to improve the policy. We’ll use the Exploring Starts approach first: - We will maintain an estimate of \\(Q(s,a)\\) for each state-action. - We will generate episodes with exploring starts (start at random state with random action to ensure all pairs seen). - After each episode, we will update \\(Q(s,a)\\) for the visited state-action pairs toward the observed returns. - Then we improve the policy greedily with respect to \\(Q\\) (make it \\(\\epsilon\\)-soft if needed to continue exploration).\nFor gambler’s problem: - State = capital (1 to 9 for goal=10). - Actions = bet amount (1 to min(s, goal-s)). You can’t bet more money than you have (s) and Yyou don’t want to bet more than needed to reach the goal (goal-s). We want to maximize the probability of reaching goal.\nLet’s implement MC Control with exploring starts for the gambler:\n\n\nShow code\nimport math\n\n# Monte Carlo Control with Exploring Starts for gambler\ndef mc_control_gambler(goal=10, p_head=0.3, episodes=10000):\n    # Initialize Q and returns tracking\n    states = range(1, goal)  # 1..goal-1\n    # actions for each state: 1..min(s, goal-s)\n    actions = {s: list(range(1, min(s, goal-s) + 1)) for s in states}\n    Q = { (s,a): 0.0 for s in states for a in actions[s] }\n    returns_sum = { (s,a): 0.0 for s in states for a in actions[s] }\n    returns_count = { (s,a): 0 for s in states for a in actions[s] }\n    # Start with arbitrary policy (say always bet 1 initially)\n    policy = { s: 1 for s in states }\n    \n    for episode_num in range(episodes):\n        # Exploring start: pick random state and random action (not terminal)\n        s0 = random.choice(list(states))\n        a0 = random.choice(actions[s0])\n        # Generate episode starting from (s0, a0)\n        episode = []\n        capital = s0\n        action = a0\n        # Apply the first action from exploring start\n        next_capital = capital + action if random.random() &lt; p_head else capital - action\n        reward = 0\n        episode.append((capital, action, reward))\n        capital = next_capital\n        # Follow current policy thereafter\n        while capital not in (0, goal):\n            a = policy[capital]\n            next_capital = capital + a if random.random() &lt; p_head else capital - a\n            reward = 0\n            episode.append((capital, a, reward))\n            capital = next_capital\n        # Episode ended\n        final_reward = 1 if capital == goal else 0\n        episode.append((capital, None, final_reward))\n        \n        # Calculate returns G and update first-visit\n        G = 0\n        visited_sa = set()\n        for state, action, reward in reversed([(x,y,z) for (x,y,z) in episode]):\n            G = reward + G\n            if action is not None:  # skip terminal state in episode\n                sa = (state, action)\n                if sa not in visited_sa:\n                    visited_sa.add(sa)\n                    returns_sum[sa] += G\n                    returns_count[sa] += 1\n                    Q[sa] = returns_sum[sa] / returns_count[sa]\n                    # Improve policy for that state\n                    # Choose action with max Q\n                    best_a = max(actions[state], key=lambda a: Q[(state,a)])\n                    policy[state] = best_a\n    return policy, Q\n\nopt_policy, opt_Q = mc_control_gambler(goal=10, p_head=0.4, episodes=50000)\nprint(\"Learned optimal policy (bet amounts) for states 1..9:\")\nfor s in range(1, 10):\n    print(f\"State {s}: bet {opt_policy[s]}\")\n\n\nLearned optimal policy (bet amounts) for states 1..9:\nState 1: bet 1\nState 2: bet 2\nState 3: bet 3\nState 4: bet 4\nState 5: bet 5\nState 6: bet 4\nState 7: bet 3\nState 8: bet 2\nState 9: bet 1\n\n\nUnderstanding Optimal Betting Strategy in the Gambler’s Problem\nWhen p &lt; 0.5 (unfair coin), the optimal betting strategy might seem counterintuitive at first. Let’s understand why making larger bets is actually optimal in this scenario.\nConsider a gambler who needs to win $3 with p = 0.4 (40% chance of winning each bet). They have two possible strategies:\n\nConservative Strategy: Bet $1 three times\n\nNeed to win all three bets to reach goal\nProbability = 0.4 * 0.4 * 0.4 = 0.064 (6.4% chance of success)\n\nAggressive Strategy: Bet $3 once\n\nNeed to win just one bet\nProbability = 0.4 (40% chance of success)\n\n\nThe aggressive strategy is clearly superior here. This illustrates a fundamental principle: when the odds are against you (p &lt; 0.5), making multiple small bets is worse than making fewer larger bets. This is because:\n\nEach bet has an expected negative return\nMore bets means more opportunities for the negative expected value to manifest\nThe law of large numbers works against you, pushing your results toward the negative expected value\nBy making fewer, larger bets, you minimize exposure to the unfavorable odds\n\nThis is why our Monte Carlo control algorithm, starting from a conservative “bet 1” policy, will eventually learn to make larger bets when p &lt; 0.5. The Q-values for larger bets will yield higher expected returns, leading to policy improvement in this direction.\nNote: Monte Carlo might need many episodes to converge on the optimal policy, especially for nontrivial p. We used exploring starts to ensure all actions are tried. Without exploring starts, we could use an ε-greedy policy improvement (on-policy MC control) where we ensure the policy occasionally tries suboptimal actions ￼.\n\nExercises\nExercise 1: Implement Monte Carlo prediction and control for the Grid World environment. Compare the learned value functions and policies with those obtained from Dynamic Programming.\nExercise 2: Apply Monte Carlo methods to learn optimal play in Blackjack, following the example in Sutton & Barto. Analyze how the learned strategy compares to basic strategy tables used by players.\nExercise 3: Implement on-policy Monte Carlo control using an \\(\\epsilon\\)-soft policy instead of exploring starts:\n\nInitialize a policy \\(\\pi\\) that is \\(\\epsilon\\)-greedy with respect to \\(Q\\), ensuring \\(P(a|s) &gt; 0\\) for all state-action pairs.\nGenerate complete episodes by following policy \\(\\pi\\).\nFor each state-action pair \\((s,a)\\) visited in the episode, update the action-value function \\(Q(s,a)\\) using the observed returns.\nImprove policy \\(\\pi\\) by making it more greedy with respect to \\(Q\\) while maintaining \\(\\epsilon\\)-softness (i.e., ensure all actions still have some non-zero probability of being selected). Compare the learning performance with the exploring starts approach."
  },
  {
    "objectID": "rl-labs/part-4-monte-carlo-methods/index.html#summary",
    "href": "rl-labs/part-4-monte-carlo-methods/index.html#summary",
    "title": "Part 4: Monte Carlo Methods",
    "section": "Summary",
    "text": "Summary\nMonte Carlo methods allow us to learn value functions and policies from experience: - We estimated values by averaging returns (the essence of Monte Carlo prediction) - We saw first-visit vs every-visit approaches, which both converge to true values - For control, we ensured sufficient exploration either via exploring starts or maintaining an ε-soft policy, then improved towards optimal by comparing action returns. - MC methods wait until an episode ends to update, which can be a downside for long episodes or continuing tasks.\nNext, we’ll explore Temporal-Difference (TD) Learning, which learns from incomplete episodes and combines ideas of Monte Carlo and DP.\n\n← Part 3 | All labs | Part 5 →"
  },
  {
    "objectID": "rl-labs/index.html",
    "href": "rl-labs/index.html",
    "title": "Reinforcement Learning Labs",
    "section": "",
    "text": "A 6-part notebook series introducing reinforcement learning concepts and methods."
  },
  {
    "objectID": "rl-labs/index.html#chapters",
    "href": "rl-labs/index.html#chapters",
    "title": "Reinforcement Learning Labs",
    "section": "Chapters",
    "text": "Chapters\n\nPart 0: Basic Probability Simulations\nPart 1: Prediction and Control\nPart 2: Multi-Armed Bandits\nPart 3: Dynamic Programming\nPart 4: Monte Carlo Methods\nPart 5: Temporal Difference Learning"
  },
  {
    "objectID": "rl-labs/part-0-basic-probability-simulations/index.html",
    "href": "rl-labs/part-0-basic-probability-simulations/index.html",
    "title": "Part 0: Basic Probability Simulations",
    "section": "",
    "text": "Understanding reinforcement learning requires a solid grasp of probability and randomness. In this first notebook, we’ll explore basic probability concepts through simple Python simulations. By the end, you should feel comfortable drawing samples from different probability distributions and understanding concepts like expected value and variance in a hands-on way. ### Introduction In reinforcement learning, an agent makes decisions in the face of uncertainty. Outcomes are often random, so we need to understand probability to predict and evaluate an agent’s performance. We’ll start with simple simulations: - Coin flips and dice rolls: Drawing samples from discrete distributions. - Continuous distributions: Sampling from distributions like uniform and normal. - Estimating probabilities: Using simulations to verify theoretical expectations (Law of Large Numbers). - Expected value: Understanding the average outcome of a random process.\nLet’s begin by simulating some basic random events.\n\nSimulating a Fair Coin Flip\n\n\nShow code\nimport random\n\n# Simulate 10 coin flips and count Heads\nnum_flips = 10\nheads_count = 0\nfor i in range(num_flips):\n    flip = random.random()  # uniform [0,1)\n    if flip &lt; 0.5:\n        heads_count += 1\n\nprint(f\"Out of {num_flips} flips, got Heads = {heads_count}\")\n\n\nOut of 10 flips, got Heads = 6\n\n\n\n\nShow code\nimport math\n\ntrials = 1000\ntotal_heads = 0\nfor t in range(trials):\n    heads = 0\n    for i in range(num_flips):\n        if random.random() &lt; 0.5:\n            heads += 1\n    total_heads += heads\n\navg_heads = total_heads / trials\nprint(f\"Average Heads in {num_flips} flips (over {trials} trials) = {avg_heads:.2f}\")\n\n\nAverage Heads in 10 flips (over 1000 trials) = 4.95\n\n\n\n\nSimulating a Six-Sided Die Roll\n\n\nShow code\noutcomes = [random.randint(1, 6) for _ in range(20)]\nprint(\"20 die rolls:\", outcomes)\nprint(\"Average roll value:\", sum(outcomes)/len(outcomes))\n\n\n20 die rolls: [4, 2, 2, 3, 6, 3, 2, 6, 5, 5, 1, 5, 3, 5, 4, 3, 4, 1, 3, 5]\nAverage roll value: 3.6\n\n\n\n\nShow code\noutcomes = random.choices([\"H\", \"T\"], weights=[0.7, 0.3], k=10)\nprint(\"10 flips of a biased 70/30 coin:\", outcomes)\nprint(\"Number of Heads:\", outcomes.count(\"H\"))\n\n\n10 flips of a biased 70/30 coin: ['T', 'T', 'H', 'H', 'T', 'H', 'H', 'T', 'H', 'T']\nNumber of Heads: 5\n\n\n\n\nSampling from Continuous Distributions\nBeyond discrete outcomes, many processes produce continuous-valued outcomes. For example, an agent’s sensor reading might be normally distributed noise, or time between events might be exponentially distributed.\nLet’s draw samples from a normal distribution with mean 0 and standard deviation 1 (standard normal). We’ll generate 5 samples:\n\n\nShow code\nfor _ in range(5):\n    sample = random.gauss(0, 1)  # or random.normalvariate(0,1)\n    print(f\"Normal sample: {sample:.3f}\")\n\n\nNormal sample: -1.252\nNormal sample: -0.559\nNormal sample: 0.812\nNormal sample: -1.397\nNormal sample: 0.005\n\n\nThis hands-on practice with probability will be valuable as we move into reinforcement learning. An RL agent’s rewards and state transitions are often random, so understanding distributions will help in designing and evaluating learning algorithms.\nIn the next notebook, we’ll create a simple gambling scenario to start applying these concepts to decision-making problems.\n\nExercise: Generate 10,000 samples from random.gauss(0,1). Compute the average and standard deviation of these samples to confirm they are close to 0 and 1, respectively. (Hint: Use sum(samples)/N for mean, and (sum((x-mean)2 for x in samples)/N)0.5 for standard deviation.)\n\n\nAll labs | Part 1 →"
  },
  {
    "objectID": "rl-labs/part-3-dynamic-programming/index.html",
    "href": "rl-labs/part-3-dynamic-programming/index.html",
    "title": "Part 3: Dynamic Programming",
    "section": "",
    "text": "Dynamic Programming (DP) methods solve reinforcement learning problems when we have a perfect model of the environment (known states, actions, transition probabilities, and rewards). DP is essentially planning: using Bellman equations to compute value functions and optimal policies. While not used directly in model-free learning, DP lays the groundwork for understanding value functions and optimality.\nKey ideas: - Policy Evaluation: Compute the value function \\(v_{\\pi}(s)\\) for a given policy π (prediction with a model). - Policy Improvement: Given value function \\(v_{\\pi}\\), improve the policy by acting greedily w.r.t. those values. - Policy Iteration: Iteratively alternate evaluation and improvement until convergence to an optimal policy - Value Iteration: A streamlined approach that combines evaluation and improvement in one step (update values towards optimality directly)\nWe’ll demonstrate these in a simple Grid World environment."
  },
  {
    "objectID": "rl-labs/part-3-dynamic-programming/index.html#grid-world-environment",
    "href": "rl-labs/part-3-dynamic-programming/index.html#grid-world-environment",
    "title": "Part 3: Dynamic Programming",
    "section": "Grid World Environment",
    "text": "Grid World Environment\nConsider a 4x4 grid (like a little board game). The agent starts in some cell and can move up, down, left, or right: - Some cells are terminal states (once reached, the episode ends). - For simplicity, let’s use the top-left (0,0) and bottom-right (3,3) as terminal states. The goal could be to reach the bottom-right corner. - Each step gives a reward of -1 (so there’s a penalty per move, which encourages finding the shortest path to a terminal).\nThis setup is similar to the example in Sutton & Barto (Gridworld), where the objective is to reach a goal in as few steps as possible (minimize cost).\nLet’s define the environment dynamics: - States: (i,j) grid positions, i,j ∈ {0,1,2,3}. Two terminal states: (0,0) and (3,3). - Actions: up, down, left, right. - Transition: deterministic moves, but if an action would take the agent off-grid, it remains in the same state. - Reward: -1 for each action (except maybe 0 at terminal since episode ends there). - Discount factor γ = 1 (we’ll treat it as an episodic task with finite horizon implicitly, focusing on minimizing steps).\nWe’ll represent the value function as a 4x4 matrix for convenience.\n(0,0)  (0,1)  (0,2)  (0,3)\n(1,0)  (1,1)  (1,2)  (1,3)\n(2,0)  (2,1)  (2,2)  (2,3)\n(3,0)  (3,1)  (3,2)  (3,3)\nActions are defined as changes to the (row, col) position: - An action is a tuple (delta_row, delta_col), where: new_row = current_row + delta_row     new_col = current_col + delta_col\n\n\nShow code\nimport numpy as np\n\n# Define grid world parameters\nrows = cols = 4\nterminal_states = [(0,0), (3,3)]\nactions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # up, down, left, right as changes in (row, col)\n\ndef is_terminal(state):\n    return state in terminal_states\n\n# Helper to get next state and reward\ndef step(state, action):\n    if is_terminal(state):\n        return state, 0  # no change if terminal\n    r, c = state\n    dr, dc = action\n    new_r, new_c = r + dr, c + dc\n    # if out of bounds, stay in same state\n    if new_r &lt; 0 or new_r &gt;= rows or new_c &lt; 0 or new_c &gt;= cols:\n        new_r, new_c = r, c\n    new_state = (new_r, new_c)\n    reward = 0 if is_terminal(new_state) else -1\n    return new_state, reward\n\n\nThe above code uses a Python dictionary comprehension to create a policy mapping for all non-terminal states in the grid world. For each state: The value \\([0.25, 0.25, 0.25, 0.25]\\) represents the probability distribution over the four possible actions (up, down, left, right).\nNow we have our environment model via step(state, action). ### Policy Evaluation (Prediction) Let’s start with an arbitrary policy \\(\\pi\\) for the non-terminal states. For example: - \\(\\pi(s) =\\) uniformly random move (0.25 each dir)\nThis is just the initial policy - it’s not a good one, but it gives us a starting point for policy iteration algorithms to improve upon.\nWe want to compute the state-value function \\(v_{\\pi}(s)\\) for all s. This can be done by solving the linear equations or iteratively applying the Bellman expectation backup: \\[v_{k+1}(s) = \\sum_a \\pi(a|s)\\sum_{s{\\prime},r} P(s{\\prime},r|s,a)[r + \\gamma v_k(s{\\prime})]\\]\nIn our deterministic case with uniform random policy, that simplifies to average over all four moves: \\[v_{k+1}(s) = \\frac{1}{4}\\sum_{a \\in \\{up,down,left,right\\}} [ -1 + v_k(s{\\prime}) ],\\] for non-terminals (because each move gives reward -1 except staying or reaching terminal yields -1 or 0 appropriately).\nWe can initialize \\(v_0(s) = 0\\) for all s and iterate until values converge (the changes become very small).\n\n\nShow code\n# Initialize value function\nV = np.zeros((rows, cols))\npolicy = { (i,j): [0.25, 0.25, 0.25, 0.25] for i in range(rows) for j in range(cols) if not is_terminal((i,j)) }\n\ngamma = 1.0\ntheta = 1e-4  # convergence threshold\niterations = 0\n\nwhile True:\n    delta = 0\n    new_V = V.copy()\n    for i in range(rows):\n        for j in range(cols):\n            s = (i,j)\n            if is_terminal(s):\n                continue\n            v = 0\n            for idx, action in enumerate(actions):\n                prob = policy[s][idx]\n                next_s, reward = step(s, action)\n                v += prob * (reward + gamma * V[next_s]) # Bellman equation for policy evaluation\n            new_V[s] = v\n            delta = max(delta, abs(v - V[s])) \n    V = new_V\n    iterations += 1\n    if delta &lt; theta:\n        break\n\nprint(f\"Policy evaluation converged in {iterations} iterations.\")\nprint(\"Value function under the random policy:\")\nprint(V)\n\n\nPolicy evaluation converged in 172 iterations.\nValue function under the random policy:\n[[  0.         -12.99893866 -18.99842728 -20.99824003]\n [-12.99893866 -16.99861452 -18.9984378  -18.99842728]\n [-18.99842728 -18.9984378  -16.99861452 -12.99893866]\n [-20.99824003 -18.99842728 -12.99893866   0.        ]]\n\n\nRunning the above code will output a 4x4 grid of values. We expect: - Terminal states remain 0. - Other values are negative (because of the -1 cost each step). - States closer to a terminal (especially \\((3,2)\\) near \\((3,3)\\)) should have higher (less negative) value because they are closer to ending the episode and stopping the penalties.\nBellman Update is the crucial part: \\[v_{k+1}(s) = \\sum_a \\pi(a|s)\\sum_{s{\\prime},r} P(s{\\prime},r|s,a)[r + \\gamma v_k(s{\\prime})]\\]\nThis is the Bellman equation for policy evaluation. It says that the new value of a state is the sum of the expected value of all possible next states, weighted by the probability of transitioning to those states and the reward received.\ndelta = max(delta, abs(v - V[s])) is the maximum change in value function at any state during the iteration.\nSuggested Exercise: Try modifying the code to: - Print the value function after each iteration to see how it evolves - Use different initial values for V instead of zeros - Experiment with different gamma values to see how it affects the final values\n\nPolicy Improvement\nNow that we have \\(v_{\\pi}\\), we can improve the policy. Policy improvement means: for each state, look at the action that would yield the highest value (one-step lookahead using \\(v_{\\pi}\\)): \\[q_{\\pi}(s,a) = \\sum_{s{\\prime},r} P(s{\\prime},r|s,a)[r + \\gamma v_{\\pi}(s{\\prime})],\\] and pick a greedy action \\(a^* = \\arg\\max_a q_{\\pi}(s,a)\\).\nIf \\(a^*\\) is better than the current policy’s action, we change the policy at s to choose \\(a^*\\) (deterministically).\nLet’s perform one policy improvement step on the uniform policy’s value function.\n\n\nShow code\n# Policy Improvement\nimproved_policy = {} # This dictionary will store the improved policy\n\nfor i in range(rows): \n    for j in range(cols): \n        s = (i,j) \n        if is_terminal(s): \n            continue\n        # Calculate Q-value for each possible action\n        q_values = [] # Will store Q(s,a) for each action\n        for action in actions: \n            next_s, reward = step(s, action) # Get the next state and reward\n            # Q(s,a) = R + γV(s')\n            q = reward + gamma * V[next_s] # Using our previously computed V table\n            q_values.append(q) \n        # Find the best action(s)\n        max_q = max(q_values) # Get the maximum q-value\n        best_actions = [idx for idx, q in enumerate(q_values) if abs(q - max_q) &lt; 1e-8] # Get the indices of the best actions (if there are multiple best actions) \n        # For deterministic policy, pick one best action\n        best_action = best_actions[0] # Pick the first best action\n        # (Alternatively, could make the policy probabilistic over best_actions)\n        action_map = {0:\"^\", 1:\"v\", 2:\"&lt;\", 3:\"&gt;\"}\n        improved_policy[s] = action_map[best_action] # Store the improved policy\n        \n# Display the improved policy in grid form \nprint(\"Improved policy (arrows indicate action):\")\npolicy_grid = [[None]*cols for _ in range(rows)] # This list will store the improved policy in grid form\nfor i in range(rows):\n    for j in range(cols):\n        if is_terminal((i,j)):\n            policy_grid[i][j] = \"T\"  # terminal\n        else:\n            policy_grid[i][j] = improved_policy[(i,j)]\nfor row in policy_grid:\n    print(\" \".join(row))\n\n\nImproved policy (arrows indicate action):\nT &lt; &lt; v\n^ ^ v v\n^ ^ v v\n^ &gt; &gt; T\n\n\nThe policy above is displayed using arrows (^ for up, v for down, &lt; for left, &gt; for right). Notice how it intuitively guides the agent toward terminal states - from each cell, the arrow typically points toward the nearest goal state at (0,0) or (3,3).\nWhat we’ve just demonstrated is how a value function can be used to derive an improved policy. Even though we started with values from a random policy, this improvement step yielded a better policy by making greedy choices based on those values.\nTo summarize, we’ve completed one iteration of Policy Iteration by: 1. Evaluating the initial random policy to get its value function 2. Improving the policy by selecting greedy actions based on that value function\nThis process can be repeated - we could evaluate this new policy and improve it again, continuing until the policy stabilizes. When no further improvements are possible, we’ve found the optimal policy.\nLet’s implement the complete Policy Iteration algorithm:\n\n\nShow code\n# Full Policy Iteration\npolicy = { (i,j): [0.25,0.25,0.25,0.25] for i in range(rows) for j in range(cols) if not is_terminal((i,j)) }\nV = np.zeros((rows, cols))\n\nstable = False # This flag will track if the policy has converged\niteration = 0 # This counter will track the number of iterations\nwhile not stable:\n    iteration += 1\n    # Policy Evaluation (simpler approach: a finite number of sweeps or until stable)\n    while True:\n        delta = 0\n        for i in range(rows):\n            for j in range(cols):\n                s = (i,j)\n                if is_terminal(s): \n                    continue\n                v = V[s]\n                new_v = 0\n                for idx, action in enumerate(actions):\n                    prob = policy[s][idx]\n                    next_s, reward = step(s, action)\n                    new_v += prob * (reward + gamma * V[next_s])\n                V[s] = new_v\n                delta = max(delta, abs(v - new_v))\n        if delta &lt; 1e-4:\n            break\n\n    # Policy Improvement\n    stable = True\n    for i in range(rows):\n        for j in range(cols):\n            s = (i,j)\n            if is_terminal(s): \n                continue\n            # current policy action (deterministic for simplicity: pick first with prob)\n            current_action = np.argmax(policy[s])\n            # find best action via one-step lookahead\n            q_values = []\n            for action in actions:\n                next_s, reward = step(s, action)\n                q_values.append(reward + gamma * V[next_s])\n            best_action = np.argmax(q_values)\n            # If policy action is not best action, update policy\n            if best_action != current_action: \n                stable = False\n            # make new policy greedy (deterministic)\n            policy[s] = [0.0]*4\n            policy[s][best_action] = 1.0\n    if stable:\n        print(f\"Policy iteration converged in {iteration} iterations.\")\n        break\n\n# Final optimal value function and policy\nprint(\"Optimal value function:\")\nprint(V)\nprint(\"Optimal policy:\")\nfor i in range(rows):\n    row_policy = []\n    for j in range(cols):\n        if is_terminal((i,j)):\n            row_policy.append(\"T\")\n        else:\n            act = np.argmax(policy[(i,j)])\n            arrow = {0:\"^\", 1:\"v\", 2:\"&lt;\", 3:\"&gt;\"}[act]\n            row_policy.append(arrow)\n    print(\" \".join(row_policy))\n\n\nPolicy iteration converged in 3 iterations.\nOptimal value function:\n[[ 0.  0. -1. -2.]\n [ 0. -1. -2. -1.]\n [-1. -2. -1.  0.]\n [-2. -1.  0.  0.]]\nOptimal policy:\nT &lt; &lt; v\n^ ^ ^ v\n^ ^ v v\n^ &gt; &gt; T\n\n\nLets break down the policy iteration algorithm. There are two main loops: 1. Outer Loop: Policy Iteration (continues until policy is stable) 2. Inner Loop: Policy Evaluation (iterates until value function converges)\nInner Convergence (Policy Evaluation): Values stabilize when the maximum change in any state’s value (delta) is below threshold (1e-4)\nOuter Convergence (Policy Iteration): Policy converges when no changes are made to the policy during an evaluation-improvement cycle.\nThis should converge in a few iterations. The optimal policy will direct the agent to the nearest terminal in the minimum number of steps (essentially a shortest path to either (0,0) or (3,3)). The optimal value function will show the negative of the distance to a terminal (since each step costs -1). For example, the value of a cell might equal \\(-d\\) where d is the Manhattan distance to the closest terminal.\nIMPORTANT: A common source of bugs is actions in the policy that are not defined in the same order as the actions list. For example, if you use np.argmax(policy[s]) to get the current action, and the actions list is [0, 1, 2, 3], but the policy is defined as a dictionary with arbitrary key order like {3: 0.25, 1: 0.25, 0: 0.25, 2: 0.25}, then the action will be incorrect since dictionaries don’t maintain order.\nHere’s a better way to structure this using an Enum or constants to ensure consistency:\nclass Action(Enum):\n    UP = 0\n    DOWN = 1\n    LEFT = 2\n    RIGHT = 3\nHowever, object oriented programming is outside the scope of this tutorial, so we won’t go into more detail here.\n\n\nValue Iteration\nWe demonstrated policy iteration above. Value Iteration is another DP approach that skips having a separate policy evaluation step. It directly updates the value function with the best possible action at each step: \\[V_{k+1}(s) = \\max_a \\sum_{s{\\prime},r} P(s{\\prime},r|s,a)[r + \\gamma V_k(s{\\prime})]\\] This is essentially applying the Bellman optimality update repeatedly.\nLet’s do value iteration for the same grid:\n\n\nShow code\n# Value Iteration\nV_vi = np.zeros((rows, cols))  # Start with all values at 0\ngamma = 1.0\ntheta = 1e-4\niters = 0\nwhile True:\n    iters += 1\n    delta = 0\n    for i in range(rows):\n        for j in range(cols):\n            s = (i,j)\n            if is_terminal(s):\n                continue\n            v = V_vi[s]  # Store current value for comparison later\n            # Bellman optimality backup\n            best_q = -float('inf') # Initialize worst possible value\n            # Compute the best possible action for this state\n            for action in actions:\n                next_s, reward = step(s, action)\n                q = reward + gamma * V_vi[next_s] # Bellman equation\n                if q &gt; best_q:\n                    best_q = q\n            V_vi[s] = best_q # Update state value to best found\n            delta = max(delta, abs(v - best_q))  # Track biggest change\n    if delta &lt; theta:\n        break # We're done!\n\nprint(f\"Value Iteration converged in {iters} sweeps.\")\nprint(\"Optimal value function (from Value Iteration):\")\nprint(V_vi)\n\n\nValue Iteration converged in 3 sweeps.\nOptimal value function (from Value Iteration):\n[[ 0.  0. -1. -2.]\n [ 0. -1. -2. -1.]\n [-1. -2. -1.  0.]\n [-2. -1.  0.  0.]]\n\n\nThis should yield the same optimal value function as policy iteration (or very close). We can extract the policy by taking the argmax actions from this value function:\n\n\nShow code\noptimal_policy = []\nfor i in range(rows):\n    row_policy = []\n    for j in range(cols):\n        s = (i,j)\n        if is_terminal(s):\n            row_policy.append(\"T\")\n        else:\n            # choose best action for this state\n            best_action = None\n            best_q = -float('inf')\n            for idx, action in enumerate(actions):\n                next_s, reward = step(s, action)\n                q = reward + gamma * V_vi[next_s]\n                if q &gt; best_q:\n                    best_q = q\n                    best_action = idx\n            arrow = {0:\"^\", 1:\"v\", 2:\"&lt;\", 3:\"&gt;\"}[best_action]\n            row_policy.append(arrow)\n    optimal_policy.append(\" \".join(row_policy))\n\nprint(\"Optimal policy (from Value Iteration):\")\nfor row in optimal_policy:\n    print(row)\n\n\nOptimal policy (from Value Iteration):\nT &lt; &lt; v\n^ ^ ^ v\n^ ^ v v\n^ &gt; &gt; T\n\n\nIt should match the earlier optimal policy.\nHowever, value iteration is more efficient than policy iteration because it doesn’t require separate policy evaluation steps. Policy Iteration has two nested loops (1) Outer loop for policy updates and (2) Inner loop for policy evaluation (which itself needs multiple sweeps). Value Iteration combines these into a single loop. Each sweep directly updates values towards optimality and converges to the optimal value function faster since it doesn’t need to evaluate the intermediate policy.\nWe used Dynamic Programming to compute an optimal policy for a known MDP: - Policy Evaluation: Computed value functions for a given policy by iterative updates until convergence - Policy Improvement: Greedily improved the policy using one-step lookahead on the value function - Policy Iteration: Repeated evaluation and improvement to reach an optimal policy - Value Iteration: Directly computed the optimal value function by iterative Bellman optimality updates\nDP methods guarantee finding the optimal policy if the model is known and the state space is not too large. However, they require full knowledge of transition probabilities and are computationally expensive for large problems. In model-free scenarios, we turn to learning methods like Monte Carlo and Temporal-Difference, which we’ll explore next.\n\n← Part 2 | All labs | Part 4 →"
  }
]