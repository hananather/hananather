{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Part 2: Multi-Armed Bandits\"\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-float:\n",
    "      collapsed: true\n",
    "      smooth-scroll: true\n",
    "    toc-title: \"On this page\"\n",
    "    code-tools: true\n",
    "    code-fold: true\n",
    "    code-summary: \"Show code\"\n",
    "execute:\n",
    "  enabled: false\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b1ae07-14f7-47eb-89c6-4c59c51b46ee",
   "metadata": {},
   "source": [
    "Multi-armed bandits (MABs) are a simpler class of problems that capture the fundamental challenge of exploration vs. exploitation in reinforcement learning. The scenario is like a gambler in front of several slot machines (\u201cone-armed bandits\u201d): each pull of a lever gives a stochastic reward. The gambler (agent) wants to maximize reward over time by choosing which machines to play, balancing trying new machines (exploration) and sticking with the best one found so far (exploitation).\n\n## Problem Setup\n- You have K slot machines (arms). Each arm, when pulled, gives a reward drawn from a fixed distribution (unknown to you). For simplicity, let\u2019s say each arm gives a reward of 1 with some probability (and 0 otherwise) \u2013 a Bernoulli bandit.\n- Your goal is to maximize the total reward over a series of pulls (trials).\n\nKey challenge: Exploration vs. Exploitation\n- Exploration: Try different arms to gain information about their payoff.\n- Exploitation: Use the information to pick the arm with the highest known reward rate so far.\n\n\nWe will implement and compare three strategies:\n1. $\\epsilon$-Greedy \u2013 with probability \u03b5, explore (random arm), otherwise exploit (best arm)\n2. Upper Confidence Bound (UCB) \u2013 select the arm with the highest upper confidence bound on reward (favoring arms with less trials so far to explore uncertainty\n3. Thompson Sampling \u2013 a Bayesian approach: maintain a distribution for each arm\u2019s success probability and sample from these to decide (naturally balancing exploration and exploitation)\n\nLet\u2019s set up a bandit problem to test these. We will create a bandit with, say, 3 arms:\n- Arm 0: win probability 0.3\n- Arm 1: win probability 0.5\n- Arm 2: win probability 0.7\n\nThe best arm is #2 (70% chance of reward), but the agents don\u2019t know that initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af49552f-f959-4c9b-8110-112241caa124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# True probabilities for each arm (unknown to agent)\n",
    "true_probs = [0.3, 0.5, 0.7]\n",
    "K = len(true_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f025500-78c0-4667-b6da-5c44374426b6",
   "metadata": {},
   "source": [
    "We\u2019ll simulate a sequence of 1000 pulls. At each step, the agent chooses an arm according to the strategy, and gets a reward 1 (with the arm\u2019s true probability) or 0. We will track:\n",
    "- The cumulative reward over time.\n",
    "- The number of pulls of each arm (to see exploration)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682c7d9f-745a-4edb-93c1-ac4e3f7d6bb0",
   "metadata": {},
   "source": [
    "### \u03b5-Greedy Strategy\n",
    "\n",
    "The \u03b5-greedy strategy is simple and widely used:\n",
    "- We maintain an estimate of each arm\u2019s value (e.g., average reward observed).\n",
    "- On each step, with probability \u03b5 choose a random arm (explore), otherwise choose the arm with highest estimated value (exploit).\n",
    "- After getting the reward, update the arm\u2019s value estimate (incremental average).\n",
    "\n",
    "Let\u2019s implement \u03b5-greedy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7535a560-2640-4677-a4b8-724a47e24f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon-Greedy results:\n",
      "Arm counts: [3024, 3220, 83756]\n",
      "Estimated values: ['0.30', '0.50', '0.70']\n",
      "Total reward: 61325\n"
     ]
    }
   ],
   "source": [
    "def run_epsilon_greedy(true_probs, epsilon=0.1, trials=1000):\n",
    "    # Initialize estimates and counts\n",
    "    counts = [0] * K # number of times each arm is sampled\n",
    "    values = [0.0] * K  # estimated value (mean reward) for each arm\n",
    "    rewards_history = []\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(1, trials+1):\n",
    "        # Decide arm\n",
    "        if random.random() < epsilon:\n",
    "            arm = random.randrange(K)  # explore\n",
    "        else:\n",
    "            # exploit (argmax of values, tie-break randomly)\n",
    "            max_val = max(values)\n",
    "            best_arms = [i for i, v in enumerate(values) if v == max_val]\n",
    "            arm = random.choice(best_arms)\n",
    "        # Pull arm\n",
    "        reward = 1 if random.random() < true_probs[arm] else 0\n",
    "        # Update counts and value estimate\n",
    "        counts[arm] += 1\n",
    "        # Incremental average update for mean:\n",
    "        values[arm] += (reward - values[arm]) / counts[arm]\n",
    "        # Track total reward\n",
    "        total_reward += reward\n",
    "        rewards_history.append(total_reward)\n",
    "    return rewards_history, counts, values\n",
    "\n",
    "# Run epsilon-greedy\n",
    "history_eps, counts_eps, values_eps = run_epsilon_greedy(true_probs, epsilon=0.1, trials=90000)\n",
    "print(\"Epsilon-Greedy results:\")\n",
    "print(\"Arm counts:\", counts_eps)\n",
    "print(\"Estimated values:\", [f\"{v:.2f}\" for v in values_eps])\n",
    "print(\"Total reward:\", history_eps[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2190f753-e831-4035-9b16-c97b6bb37037",
   "metadata": {},
   "source": [
    "**Question**: What happens if \u03b5 is too high or too low?\n",
    "Feel free to try different \u03b5 values in `run_epsilon_greedy` to see the effect on total reward.\n",
    "\n",
    "### Upper Confidence Bound (UCB) Strategy\n",
    "UCB is a more advanced strategy that addresses a limitation of \u03b5-greedy: \u03b5-greedy explores blindly. UCB uses a confidence interval approach: it picks the arm with the highest upper confidence bound on the estimated reward.One common formula (UCB1) for each arm a at time t is:\n",
    "$$\n",
    "\\text{UCB}_a = \\hat{Q}_a + \\sqrt{\\frac{2 \\ln t}{N_a}}\n",
    "$$\n",
    "where:\n",
    "- $\\hat{Q}_a$ is the current estimated value (mean reward) of arm a.\n",
    "- $N_a$ is how many times arm a has been pulled.\n",
    "- $N_a$ is how many times arm a has been pulled.\n",
    "\n",
    "We will implement UCB1. Note: We need to pull each arm at least once initially to get an estimate (and avoid division by zero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bed98226-4afd-4fbd-9da7-47eb2a56077f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "UCB results:\n",
      "Arm counts: [71, 136, 793]\n",
      "Estimated values: ['0.37', '0.49', '0.68']\n",
      "Total reward: 634\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def run_ucb(trials=1000):\n",
    "    counts = [0] * K\n",
    "    values = [0.0] * K \n",
    "    rewards_history = []\n",
    "    total_reward = 0\n",
    "\n",
    "    # Initial phase: play each arm once\n",
    "    for arm in range(K):\n",
    "        reward = 1 if random.random() < true_probs[arm] else 0\n",
    "        counts[arm] += 1\n",
    "        values[arm] += reward  # single sample mean is the reward itself\n",
    "        total_reward += reward\n",
    "        rewards_history.append(total_reward)\n",
    "    # Now do the remaining trials\n",
    "    for t in range(K+1, trials+1):\n",
    "        # Compute UCB for each arm\n",
    "        ucb_values = []\n",
    "        for arm in range(K):\n",
    "            # Exploitation term = values[arm]\n",
    "            # Exploration term = sqrt(2 * ln(t) / counts[arm])\n",
    "            exploration_bonus = math.sqrt(2 * math.log(t) / counts[arm])\n",
    "            ucb_values.append(values[arm] + exploration_bonus)\n",
    "        # Select arm with highest UCB\n",
    "        arm = ucb_values.index(max(ucb_values))\n",
    "        # Pull arm\n",
    "        reward = 1 if random.random() < true_probs[arm] else 0\n",
    "        counts[arm] += 1\n",
    "        # Update mean estimate\n",
    "        values[arm] += (reward - values[arm]) / counts[arm] # incremental mean update\n",
    "        total_reward += reward\n",
    "        rewards_history.append(total_reward)\n",
    "    return rewards_history, counts, values\n",
    "\n",
    "history_ucb, counts_ucb, values_ucb = run_ucb(trials=1000)\n",
    "print(\"\\nUCB results:\")\n",
    "print(\"Arm counts:\", counts_ucb)\n",
    "print(\"Estimated values:\", [f\"{v:.2f}\" for v in values_ucb])\n",
    "print(\"Total reward:\", history_ucb[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32c4cce-fda3-4690-90cc-d49fc9b748b2",
   "metadata": {},
   "source": [
    "The formula `values[arm] += (reward - values[arm]) / counts[arm]` keeps a running average in the UCB algorithm. If you have an old average `m` over `n` samples, and you add a new value `x`, the new average is `(m * n + x) / (n + 1)`. This pattern pops up elsewhere\u2014reinforcement learning, online learning, even basic statistics.\n",
    "\n",
    "\n",
    "## 3 Thompson Sampling\n",
    "Thompson Sampling is a Bayesian approach:\n",
    "- Assume a prior distribution for each arm's success probability. For a Bernoulli reward, a common prior is Beta(1,1) (uniform).\n",
    "- After each pull, update the posterior for that arm (Beta prior updated with observed successes/failures).\n",
    "- To choose an arm: sample a success probability from each arm's current posterior, then pick the arm with the highest sample. This way, arms with more uncertainty or higher potential can win the selection sometimes, naturally balancing exploration and exploitation.\n",
    "\n",
    "We'll implement Thompson Sampling using Beta distributions for each arm. Python's `random.betavariate(alpha, beta)` can sample from a Beta distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6839a014-d8b4-493b-a1c4-9369cbbfa181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thompson Sampling results:\n",
      "Arm counts: [6, 27, 967]\n",
      "Estimated values: ['0.00', '0.41', '0.72']\n",
      "Total reward: 704\n"
     ]
    }
   ],
   "source": [
    "def run_thompson(trials=10000):\n",
    "    # Start with prior Beta(1,1) for each arm\n",
    "    alpha = [1] * K\n",
    "    beta = [1] * K\n",
    "    counts = [0] * K\n",
    "    values = [0.0] * K  # can track mean as successes / trials for info\n",
    "    total_reward = 0\n",
    "    rewards_history = []\n",
    "\n",
    "    for t in range(1, trials+1):\n",
    "        # Sample a probability for each arm from its Beta(alpha, beta)\n",
    "        sampled_probs = [random.betavariate(alpha[a], beta[a]) for a in range(K)]\n",
    "        arm = sampled_probs.index(max(sampled_probs))\n",
    "        # Pull the chosen arm\n",
    "        reward = 1 if random.random() < true_probs[arm] else 0\n",
    "        counts[arm] += 1\n",
    "        # Update alpha, beta (posterior update: alpha += reward, beta += (1-reward))\n",
    "        alpha[arm] += reward\n",
    "        beta[arm] += (1 - reward)\n",
    "        # Update value estimate for tracking\n",
    "        values[arm] += (reward - values[arm]) / counts[arm]\n",
    "        total_reward += reward\n",
    "        rewards_history.append(total_reward)\n",
    "    return rewards_history, counts, values\n",
    "\n",
    "history_th, counts_th, values_th = run_thompson(trials=1000)\n",
    "print(\"\\nThompson Sampling results:\")\n",
    "print(\"Arm counts:\", counts_th)\n",
    "print(\"Estimated values:\", [f\"{v:.2f}\" for v in values_th])\n",
    "print(\"Total reward:\", history_th[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c664cadd-4ebb-4326-86d1-ceb8d96296f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average reward per pull:\n",
      "Epsilon-Greedy: 0.681\n",
      "UCB: 0.634\n",
      "Thompson Sampling: 0.704\n"
     ]
    }
   ],
   "source": [
    "avg_reward_eps = history_eps[-1] / len(history_eps)\n",
    "avg_reward_ucb = history_ucb[-1] / len(history_ucb)\n",
    "avg_reward_th = history_th[-1] / len(history_th)\n",
    "print(\"\\nAverage reward per pull:\")\n",
    "print(f\"Epsilon-Greedy: {avg_reward_eps:.3f}\")\n",
    "print(f\"UCB: {avg_reward_ucb:.3f}\")\n",
    "print(f\"Thompson Sampling: {avg_reward_th:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c65242-9ae4-461d-9b40-911aacbb4589",
   "metadata": {},
   "source": [
    "- \u03b5-Greedy: Simple, ensures some exploration. But it doesn't reduce exploration over time (unless \u03b5 is decayed), so it might waste some pulls exploring even when fairly confident\n",
    "- UCB: Uses a principled approach to exploration. It explores less over time as it gains confidence, focusing on uncertain arms until confidence bounds narrow\n",
    "- Thompson Sampling: A Bayesian method that often performs as well or better by naturally balancing exploration according to uncertainty. It's a bit more complex (requires sampling and thinking in terms of distributions), but very powerful.\n",
    "\n",
    "\n",
    "\n",
    "All these methods are important in reinforcement learning and online decision-making. They illustrate the core exploration-exploitation trade-off in a simple setting (no state transitions, just repeated actions). In a full RL problem, similar concepts apply when an agent tries actions in an environment and must explore new actions while exploiting what it knows.Next, we will move to **Dynamic Programming** for RL, where we assume we know the environment dynamics and see how to compute optimal policies by planning (instead of learning from data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n",
    "[\u2190 Part 1](../part-1-prediction-and-control/) | [All labs](../) | [Part 3 \u2192](../part-3-dynamic-programming/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}