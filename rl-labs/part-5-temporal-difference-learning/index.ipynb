{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Part 5: Temporal Difference Learning\"\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-float:\n",
    "      collapsed: true\n",
    "      smooth-scroll: true\n",
    "    toc-title: \"On this page\"\n",
    "    code-tools: true\n",
    "    code-fold: true\n",
    "    code-summary: \"Show code\"\n",
    "execute:\n",
    "  enabled: false\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal-Difference (TD) learning is a powerful model-free approach that blends Monte Carlo and Dynamic Programming ideas. Like MC, TD learns from raw experience without a model. Like DP, TD updates estimates based partly on other learned estimates (bootstrapping). This allows TD to learn online after each step, without waiting for the episode to finish.\nKey concepts:\n- TD(0) Prediction: Also known as one-step TD or TD(0), updates the value $V(s)$ toward the observed reward plus the value of the next state:\n$$V(s) \\leftarrow V(s) + \\alpha [R_{t+1} + \\gamma V(s_{t+1}) - V(s)]$$ \nThis update is based on the TD error $\\delta = R_{t+1} + \\gamma V(s_{t+1}) - V(s)$.\n\n\n- SARSA (on-policy TD control): Updates action-value $Q(s,a)$ toward $R_{t+1} + \\gamma Q(s_{t+1}, a_{t+1})$, using the action actually taken next (following current policy)\n- Q-Learning (off-policy TD control): Updates $Q(s,a)$ toward $R_{t+1} + \\gamma \\max_{a\u2019} Q(s_{t+1}, a\u2019)$, using the greedy best action for the next state (target policy is different from behavior)\n- On-policy vs Off-policy: SARSA is on-policy (learns value of the policy it follows, including its exploration) whereas Q-learning is off-policy (learns value of the optimal policy independent of the agent\u2019s behavior).\n\nLet\u2019s illustrate TD(0) prediction first, then implement SARSA and Q-learning for a control task.\n\n### TD(0) Prediction Example\n\nWe\u2019ll use the gambler\u2019s problem (or we could use grid world) and see how TD updates converge to true values.\n\nLet\u2019s do TD(0) for gambler policy \u201cbet 1\u201d, as we did with MC, and see if it approximates the value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TD(0) estimated value of state 5 (starting state): 0.515\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "def td0_prediction(policy_func, alpha=0.1, episodes=1000, start_capital=5, goal=10, p_head=0.5):\n",
    "    # Initialize value estimates\n",
    "    V = {s: 0.0 for s in range(goal+1)}\n",
    "    V[goal] = 1.0\n",
    "    V[0] = 0.0\n",
    "    for ep in range(episodes):\n",
    "        # start at given start_capital each episode\n",
    "        capital = start_capital\n",
    "        while capital not in (0, goal):\n",
    "            a = policy_func(capital)\n",
    "            next_capital = capital + a if random.random() < p_head else capital - a\n",
    "            reward = 0\n",
    "            # TD update\n",
    "            V[capital] += alpha * (reward + V[next_capital] - V[capital])\n",
    "            capital = next_capital\n",
    "        # Terminal state update (the last transition gives reward 1 or 0)\n",
    "        # Actually, we can handle the terminal step when loop breaks:\n",
    "        if capital == goal:\n",
    "            # If we consider the step into terminal yielding reward 1:\n",
    "            # last state before terminal (call it prev) would have gotten update in loop as:\n",
    "            # V(prev) += alpha * (1 + 0 - V(prev))\n",
    "            # But since we broke out after reaching terminal, let's simulate the last update:\n",
    "            pass\n",
    "    return V\n",
    "\n",
    "td_values = td0_prediction(policy_func=lambda s: 1, alpha=0.1, episodes=10000, start_capital=5, goal=10, p_head=0.5)\n",
    "print(\"TD(0) estimated value of state 5 (starting state):\", round(td_values[5], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Code Explanation\n",
    "\n",
    "- we start with some capital (default 5)\n",
    "- our goal is to reach a certain amount (default 10)\n",
    "- each round you can bet any amount up to your current capital\n",
    "- you have a 50% chance of winning (p_head=0.5)\n",
    "If you win, you get the amount you bet; if you lose, you lose the amount you bet. You want to find the optimal betting strategy\n",
    "\n",
    "We might need to refine handling of the terminal reward. But essentially, TD(0) will update on each step towards the value of the next state. Over many episodes, it should converge to the true $v_{\\pi}(5) \\approx 0.5$. You can try different starting states or do multiple start states in episodes to cover more states.\n",
    "\n",
    "Think of it like a gambler trying to turn $5 into $10 by making a series of bets. The value function $V(s)$ represents the probability of winning (reaching $10) starting from state $s$ under the current policy.\n",
    "\n",
    "TD has an advantage of updating during the episode, and it can learn from incomplete episodes (if we use continuing tasks or bootstrapping without waiting for termination).\n",
    "\n",
    "Now, let\u2019s move to control with SARSA and Q-learning, which are more commonly demonstrated on tasks like Grid World or Cliff Walking.\n",
    "\n",
    "\n",
    "## SARSA vs Q-Learning on Grid World\n",
    "\n",
    "We\u2019ll use the grid world, but introduce a slight twist: let\u2019s add a \u201cbad state\u201d to illustrate the difference between on-policy and off-policy. A classic example is the \u201cCliff\u201d environment (Sutton & Barto Example 6.6) where following the optimal path has risk of falling off a cliff (big negative reward) vs a safer path.\n",
    "\n",
    "For simplicity, let\u2019s create a small grid where:\n",
    "- Start at (0,0), goal at (0,3).\n",
    "- There\u2019s a \u201ccliff\u201d at positions (1,1) for example that gives a large negative reward if stepped into.\n",
    "- The optimal policy might go around the cliff in an on-policy method vs jump over in off-policy.\n",
    "\n",
    "**SARSA and Q-learning:**\n",
    "- SARSA will update using the action it actually takes next (which, if it\u2019s following $\\epsilon$-greedy, might be suboptimal, so it ends up learning the value of its exploratory policy).\n",
    "- Q-learning will update as if it always takes the optimal next action (even if it didn\u2019t), thus it tends to learn optimistic values and converge to the true optimal policy value.\n",
    "\n",
    "Let\u2019s still do a simple grid with some penalty state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rows, grid_cols = 3, 4\n",
    "start_state = (0,0)\n",
    "goal_state = (0,3)\n",
    "cliff_state = (1,2)  # stepping here gives -10 reward (for example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a step function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_step(state, action):\n",
    "    if state == goal_state:\n",
    "        return state, 0  # absorbing\n",
    "    r, c = state\n",
    "    dr, dc = action\n",
    "    nr, nc = r+dr, c+dc\n",
    "    # bounds check\n",
    "    if nr < 0 or nr >= grid_rows or nc < 0 or nc >= grid_cols:\n",
    "        nr, nc = r, c  # hit wall, stay (could also give a small neg reward)\n",
    "    new_state = (nr, nc)\n",
    "    # reward logic\n",
    "    if new_state == cliff_state:\n",
    "        reward = -10\n",
    "    elif new_state == goal_state:\n",
    "        reward = +10\n",
    "    else:\n",
    "        reward = -1  # each step cost\n",
    "    return new_state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define actions and run SARSA and Q-learning episodes.\n",
    "\n",
    "We\u2019ll keep it short and not do too many episodes for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy policy from SARSA:\n",
      "< > > G\n",
      "^ ^ ^ ^\n",
      "^ ^ < <\n",
      "\n",
      "Greedy policy from Q-learning:\n",
      "> > > G\n",
      "^ ^ ^ ^\n",
      "v > < ^\n"
     ]
    }
   ],
   "source": [
    "actions = [(0,1),(0,-1),(1,0),(-1,0)]  # right, left, down, up moves\n",
    "def epsilon_greedy_action(Q, state, epsilon=0.1):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(actions)\n",
    "    else:\n",
    "        # choose best action from Q\n",
    "        q_values = [Q.get((state,a), 0) for a in actions]\n",
    "        max_q = max(q_values)\n",
    "        best_actions = [a for a, q in zip(actions, q_values) if q == max_q]\n",
    "        return random.choice(best_actions)\n",
    "\n",
    "def run_sarsa(alpha=0.5, gamma=1.0, epsilon=0.1, episodes=1000):\n",
    "    Q = {}\n",
    "    for ep in range(episodes):\n",
    "        state = start_state\n",
    "        action = epsilon_greedy_action(Q, state, epsilon)\n",
    "        while state != goal_state:\n",
    "            next_state, reward = grid_step(state, action)\n",
    "            next_action = epsilon_greedy_action(Q, next_state, epsilon)\n",
    "            # Q(s,a) update\n",
    "            current_q = Q.get((state, action), 0.0)\n",
    "            next_q = Q.get((next_state, next_action), 0.0)\n",
    "            td_target = reward + gamma * next_q\n",
    "            Q[(state, action)] = current_q + alpha * (td_target - current_q)\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "    return Q\n",
    "\n",
    "def run_q_learning(alpha=0.5, gamma=1.0, epsilon=0.1, episodes=1000):\n",
    "    Q = {}\n",
    "    for ep in range(episodes):\n",
    "        state = start_state\n",
    "        while state != goal_state:\n",
    "            action = epsilon_greedy_action(Q, state, epsilon)\n",
    "            next_state, reward = grid_step(state, action)\n",
    "            # Q-learning update\n",
    "            current_q = Q.get((state, action), 0.0)\n",
    "            # next state's best action value\n",
    "            next_q_values = [Q.get((next_state,a), 0.0) for a in actions]\n",
    "            next_max = max(next_q_values) if next_q_values else 0.0\n",
    "            td_target = reward + gamma * next_max\n",
    "            Q[(state, action)] = current_q + alpha * (td_target - current_q)\n",
    "            state = next_state\n",
    "    return Q\n",
    "\n",
    "# Run SARSA and Q-learning\n",
    "Q_sarsa = run_sarsa(episodes=1000)\n",
    "Q_qlearn = run_q_learning(episodes=1000)\n",
    "\n",
    "# Derive policies from Q (greedy)\n",
    "policy_sarsa = {}\n",
    "policy_qlearn = {}\n",
    "for r in range(grid_rows):\n",
    "    for c in range(grid_cols):\n",
    "        s = (r,c)\n",
    "        # skip goal\n",
    "        if s == goal_state:\n",
    "            continue\n",
    "        # find best action in Q for state\n",
    "        q_vals_sarsa = {a: Q_sarsa.get((s,a), 0.0) for a in actions}\n",
    "        q_vals_q = {a: Q_qlearn.get((s,a), 0.0) for a in actions}\n",
    "        best_a_sarsa = max(q_vals_sarsa, key=q_vals_sarsa.get)\n",
    "        best_a_q = max(q_vals_q, key=q_vals_q.get)\n",
    "        policy_sarsa[s] = best_a_sarsa\n",
    "        policy_qlearn[s] = best_a_q\n",
    "\n",
    "print(\"Greedy policy from SARSA:\")\n",
    "for r in range(grid_rows):\n",
    "    row = []\n",
    "    for c in range(grid_cols):\n",
    "        s = (r,c)\n",
    "        if s == goal_state:\n",
    "            row.append(\"G\")\n",
    "        elif policy_sarsa.get(s) is None:\n",
    "            row.append(\".\")\n",
    "        else:\n",
    "            arrow = { (0,1): \">\", (0,-1): \"<\", (1,0): \"v\", (-1,0): \"^\" }\n",
    "            row.append(arrow[policy_sarsa[s]])\n",
    "    print(\" \".join(row))\n",
    "\n",
    "print(\"\\nGreedy policy from Q-learning:\")\n",
    "for r in range(grid_rows):\n",
    "    row = []\n",
    "    for c in range(grid_cols):\n",
    "        s = (r,c)\n",
    "        if s == goal_state:\n",
    "            row.append(\"G\")\n",
    "        elif policy_qlearn.get(s) is None:\n",
    "            row.append(\".\")\n",
    "        else:\n",
    "            arrow = { (0,1): \">\", (0,-1): \"<\", (1,0): \"v\", (-1,0): \"^\" }\n",
    "            row.append(arrow[policy_qlearn[s]])\n",
    "    print(\" \".join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On-Policy vs. Off-Policy\n",
    "\n",
    "Consider the general **Temporal Difference (TD)** update:\n",
    "\n",
    "$$\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha\\Bigl[\\underbrace{r + \\gamma Q(s', a')}_{\\text{on-policy (SARSA)}} - Q(s,a)\\Bigr]\n",
    "$$\n",
    "\n",
    "The critical point making this **on-policy** is that the value of the next state-action pair \\(Q(s', a')\\) comes directly from the action \\(a'\\) **actually taken by the current policy**, which could include exploration.\n",
    "\n",
    "In contrast, the **Q-Learning** update is:\n",
    "\n",
    "$$\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha\\Bigl[\\underbrace{r + \\gamma \\max_{a'}Q(s', a')}_{\\text{off-policy (Q-Learning)}} - Q(s,a)\\Bigr]\n",
    "$$\n",
    "\n",
    "Here, the critical off-policy part is clearly marked by the presence of the \\(\\max_{a'}Q(s',a')\\). This means the update is computed using the best possible next action, regardless of the actual exploratory action chosen by the current policy. Thus, the policy being evaluated (greedy) differs from the behavior policy used to select actions (e.g., epsilon-greedy).\n",
    "\n",
    "In our above code:\n",
    "\n",
    "- **SARSA** explicitly selects the next action (`next_action`) via the same policy used for exploration (epsilon-greedy), thus directly using this action's value:\n",
    "\n",
    "```python\n",
    "td_target = reward + gamma * Q.get((next_state, next_action), 0.0)\n",
    "```\n",
    "\n",
    "- **Q-Learning** does not directly select an action via exploration for updating, but instead takes the maximum value of the next state's Q-values:\n",
    "\n",
    "```python\n",
    "next_max = max([Q.get((next_state, a), 0.0) for a in actions])\n",
    "td_target = reward + gamma * next_max\n",
    "```\n",
    "\n",
    "This explicitly demonstrates why SARSA is on-policy (next action is sampled from the policy being improved), while Q-learning is off-policy (next action used in the update is always the greedy, optimal action, not necessarily chosen by the exploratory policy).\n",
    "\n",
    "\n",
    "To reiterate:\n",
    "- On-policy methods (SARSA) learn about the policy being executed, which includes exploration. They generally result in safer learning (they don\u2019t propagate unrealistically high values for risky states because those high values won\u2019t materialize under the $\\epsilon$-greedy behavior that might fall off).\n",
    "- Off-policy (Q-learning) learns the optimal target policy independent of the current behavior. It can converge to the optimal faster, but one must be careful with convergence conditions (need sufficient exploration, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n",
    "[\u2190 Part 4](../part-4-monte-carlo-methods/) | [All labs](../)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}