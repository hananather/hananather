{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Part 3: Dynamic Programming\"\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-float:\n",
    "      collapsed: true\n",
    "      smooth-scroll: true\n",
    "    toc-title: \"On this page\"\n",
    "    code-tools: true\n",
    "    code-fold: true\n",
    "    code-summary: \"Show code\"\n",
    "execute:\n",
    "  enabled: false\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa11f56c-0f49-4fd2-9f85-02b14e23bf81",
   "metadata": {},
   "source": [
    "Dynamic Programming (DP) methods solve reinforcement learning problems when we have a perfect model of the environment (known states, actions, transition probabilities, and rewards). DP is essentially planning: using Bellman equations to compute value functions and optimal policies. While not used directly in model-free learning, DP lays the groundwork for understanding value functions and optimality.\n\n\nKey ideas:\n- Policy Evaluation: Compute the value function $v_{\\pi}(s)$ for a given policy \u03c0 (prediction with a model).\n- Policy Improvement: Given value function $v_{\\pi}$, improve the policy by acting greedily w.r.t. those values.\n- Policy Iteration: Iteratively alternate evaluation and improvement until convergence to an optimal policy\n- Value Iteration: A streamlined approach that combines evaluation and improvement in one step (update values towards optimality directly)\n  \nWe'll demonstrate these in a simple Grid World environment.\n\n## Grid World Environment\nConsider a 4x4 grid (like a little board game). The agent starts in some cell and can move up, down, left, or right:\n- Some cells are terminal states (once reached, the episode ends).\n- For simplicity, let's use the top-left (0,0) and bottom-right (3,3) as terminal states. The goal could be to reach the bottom-right corner.\n- Each step gives a reward of -1 (so there's a penalty per move, which encourages finding the shortest path to a terminal).\n\nThis setup is similar to the example in Sutton & Barto (Gridworld), where the objective is to reach a goal in as few steps as possible (minimize cost).\n\nLet's define the environment dynamics:\n- States: (i,j) grid positions, i,j \u2208 {0,1,2,3}. Two terminal states: (0,0) and (3,3).\n- Actions: up, down, left, right.\n- Transition: deterministic moves, but if an action would take the agent off-grid, it remains in the same state.\n- Reward: -1 for each action (except maybe 0 at terminal since episode ends there).\n- Discount factor \u03b3 = 1 (we'll treat it as an episodic task with finite horizon implicitly, focusing on minimizing steps).\n\nWe'll represent the value function as a 4x4 matrix for convenience.\n\n```\n(0,0)  (0,1)  (0,2)  (0,3)\n(1,0)  (1,1)  (1,2)  (1,3)\n(2,0)  (2,1)  (2,2)  (2,3)\n(3,0)  (3,1)  (3,2)  (3,3)\n```\nActions are defined as changes to the `(row, col)` position:\n- An action is a tuple `(delta_row, delta_col)`, where:\n  ```\n    new_row = current_row + delta_row\n    new_col = current_col + delta_col\n  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d0bb3ae-ab6d-48a1-9f24-ace30e7b8e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define grid world parameters\n",
    "rows = cols = 4\n",
    "terminal_states = [(0,0), (3,3)]\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # up, down, left, right as changes in (row, col)\n",
    "\n",
    "def is_terminal(state):\n",
    "    return state in terminal_states\n",
    "\n",
    "# Helper to get next state and reward\n",
    "def step(state, action):\n",
    "    if is_terminal(state):\n",
    "        return state, 0  # no change if terminal\n",
    "    r, c = state\n",
    "    dr, dc = action\n",
    "    new_r, new_c = r + dr, c + dc\n",
    "    # if out of bounds, stay in same state\n",
    "    if new_r < 0 or new_r >= rows or new_c < 0 or new_c >= cols:\n",
    "        new_r, new_c = r, c\n",
    "    new_state = (new_r, new_c)\n",
    "    reward = 0 if is_terminal(new_state) else -1\n",
    "    return new_state, reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca3c043-1d18-4b1b-9482-295fe95a01bc",
   "metadata": {},
   "source": [
    "The above code uses a Python dictionary comprehension to create a policy mapping for all non-terminal states in the grid world.\n",
    "For each state:\n",
    "The value $[0.25, 0.25, 0.25, 0.25]$ represents the probability distribution over the four possible actions (up, down, left, right).\n",
    "\n",
    "\n",
    "Now we have our environment model via `step(state, action)`.\n",
    "### Policy Evaluation (Prediction)\n",
    "Let\u2019s start with an arbitrary policy $\\pi$ for the non-terminal states. For example:\n",
    "- $\\pi(s) =$ uniformly random move (0.25 each dir)\n",
    "\n",
    "This is just the initial policy - it's not a good one, but it gives us a starting point for policy iteration algorithms to improve upon.\n",
    "\n",
    "We want to compute the state-value function $v_{\\pi}(s)$ for all s. This can be done by solving the linear equations or iteratively applying the Bellman expectation backup:\n",
    "$$v_{k+1}(s) = \\sum_a \\pi(a|s)\\sum_{s{\\prime},r} P(s{\\prime},r|s,a)[r + \\gamma v_k(s{\\prime})]$$\n",
    "\n",
    "In our deterministic case with uniform random policy, that simplifies to average over all four moves:\n",
    "$$v_{k+1}(s) = \\frac{1}{4}\\sum_{a \\in \\{up,down,left,right\\}} [ -1 + v_k(s{\\prime}) ],$$ \n",
    "for non-terminals (because each move gives reward -1 except staying or reaching terminal yields -1 or 0 appropriately).\n",
    "\n",
    "We can initialize $v_0(s) = 0$ for all s and iterate until values converge (the changes become very small)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90218c83-51d0-4bb8-8720-0d475c86ce34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluation converged in 172 iterations.\n",
      "Value function under the random policy:\n",
      "[[  0.         -12.99893866 -18.99842728 -20.99824003]\n",
      " [-12.99893866 -16.99861452 -18.9984378  -18.99842728]\n",
      " [-18.99842728 -18.9984378  -16.99861452 -12.99893866]\n",
      " [-20.99824003 -18.99842728 -12.99893866   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize value function\n",
    "V = np.zeros((rows, cols))\n",
    "policy = { (i,j): [0.25, 0.25, 0.25, 0.25] for i in range(rows) for j in range(cols) if not is_terminal((i,j)) }\n",
    "\n",
    "gamma = 1.0\n",
    "theta = 1e-4  # convergence threshold\n",
    "iterations = 0\n",
    "\n",
    "while True:\n",
    "    delta = 0\n",
    "    new_V = V.copy()\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            s = (i,j)\n",
    "            if is_terminal(s):\n",
    "                continue\n",
    "            v = 0\n",
    "            for idx, action in enumerate(actions):\n",
    "                prob = policy[s][idx]\n",
    "                next_s, reward = step(s, action)\n",
    "                v += prob * (reward + gamma * V[next_s]) # Bellman equation for policy evaluation\n",
    "            new_V[s] = v\n",
    "            delta = max(delta, abs(v - V[s])) \n",
    "    V = new_V\n",
    "    iterations += 1\n",
    "    if delta < theta:\n",
    "        break\n",
    "\n",
    "print(f\"Policy evaluation converged in {iterations} iterations.\")\n",
    "print(\"Value function under the random policy:\")\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51cb7fc-fd24-40b8-bfff-87c36d688536",
   "metadata": {},
   "source": [
    "Running the above code will  output a 4x4 grid of values. We expect:\n",
    "- Terminal states remain 0.\n",
    "- Other values are negative (because of the -1 cost each step).\n",
    "- States closer to a terminal (especially $(3,2)$ near $(3,3)$) should have higher (less negative) value because they are closer to ending the episode and stopping the penalties.\n",
    "\n",
    "\n",
    "**Bellman Update** is the crucial part:\n",
    "$$v_{k+1}(s) = \\sum_a \\pi(a|s)\\sum_{s{\\prime},r} P(s{\\prime},r|s,a)[r + \\gamma v_k(s{\\prime})]$$\n",
    "\n",
    "This is the Bellman equation for policy evaluation. It says that the new value of a state is the sum of the expected value of all possible next states, weighted by the probability of transitioning to those states and the reward received.\n",
    "\n",
    "`delta = max(delta, abs(v - V[s]))` is the maximum change in value function at any state during the iteration.\n",
    "\n",
    "**Suggested Exercise:**\n",
    "Try modifying the code to:\n",
    "- Print the value function after each iteration to see how it evolves\n",
    "- Use different initial values for V instead of zeros\n",
    "- Experiment with different gamma values to see how it affects the final values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2764b4-24fd-4c83-919f-e9e0a652734d",
   "metadata": {},
   "source": [
    "### Policy Improvement\n",
    "\n",
    "Now that we have $v_{\\pi}$, we can improve the policy. Policy improvement means: for each state, look at the action that would yield the highest value (one-step lookahead using $v_{\\pi}$):\n",
    "$$q_{\\pi}(s,a) = \\sum_{s{\\prime},r} P(s{\\prime},r|s,a)[r + \\gamma v_{\\pi}(s{\\prime})],$$\n",
    "and pick a greedy action $a^* = \\arg\\max_a q_{\\pi}(s,a)$.\n",
    "\n",
    "If $a^*$ is better than the current policy\u2019s action, we change the policy at s to choose $a^*$ (deterministically).\n",
    "\n",
    "Let\u2019s perform one policy improvement step on the uniform policy\u2019s value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d799b5ea-1feb-4203-be92-64ea1c35873e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved policy (arrows indicate action):\n",
      "T < < v\n",
      "^ ^ v v\n",
      "^ ^ v v\n",
      "^ > > T\n"
     ]
    }
   ],
   "source": [
    "# Policy Improvement\n",
    "improved_policy = {} # This dictionary will store the improved policy\n",
    "\n",
    "for i in range(rows): \n",
    "    for j in range(cols): \n",
    "        s = (i,j) \n",
    "        if is_terminal(s): \n",
    "            continue\n",
    "        # Calculate Q-value for each possible action\n",
    "        q_values = [] # Will store Q(s,a) for each action\n",
    "        for action in actions: \n",
    "            next_s, reward = step(s, action) # Get the next state and reward\n",
    "            # Q(s,a) = R + \u03b3V(s')\n",
    "            q = reward + gamma * V[next_s] # Using our previously computed V table\n",
    "            q_values.append(q) \n",
    "        # Find the best action(s)\n",
    "        max_q = max(q_values) # Get the maximum q-value\n",
    "        best_actions = [idx for idx, q in enumerate(q_values) if abs(q - max_q) < 1e-8] # Get the indices of the best actions (if there are multiple best actions) \n",
    "        # For deterministic policy, pick one best action\n",
    "        best_action = best_actions[0] # Pick the first best action\n",
    "        # (Alternatively, could make the policy probabilistic over best_actions)\n",
    "        action_map = {0:\"^\", 1:\"v\", 2:\"<\", 3:\">\"}\n",
    "        improved_policy[s] = action_map[best_action] # Store the improved policy\n",
    "        \n",
    "# Display the improved policy in grid form \n",
    "print(\"Improved policy (arrows indicate action):\")\n",
    "policy_grid = [[None]*cols for _ in range(rows)] # This list will store the improved policy in grid form\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        if is_terminal((i,j)):\n",
    "            policy_grid[i][j] = \"T\"  # terminal\n",
    "        else:\n",
    "            policy_grid[i][j] = improved_policy[(i,j)]\n",
    "for row in policy_grid:\n",
    "    print(\" \".join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536899fb-6351-499c-849e-f668e79ccad3",
   "metadata": {},
   "source": [
    "The policy above is displayed using arrows (^ for up, v for down, < for left, > for right). Notice how it intuitively guides the agent toward terminal states - from each cell, the arrow typically points toward the nearest goal state at (0,0) or (3,3).\n",
    "\n",
    "What we've just demonstrated is how a value function can be used to derive an improved policy. Even though we started with values from a random policy, this improvement step yielded a better policy by making greedy choices based on those values.\n",
    "\n",
    "To summarize, we've completed one iteration of Policy Iteration by:\n",
    "1. Evaluating the initial random policy to get its value function\n",
    "2. Improving the policy by selecting greedy actions based on that value function\n",
    "\n",
    "This process can be repeated - we could evaluate this new policy and improve it again, continuing until the policy stabilizes. When no further improvements are possible, we've found the optimal policy.\n",
    "\n",
    "Let's implement the complete Policy Iteration algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62d728e0-901a-4e83-bfdb-8a918d9a66eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy iteration converged in 3 iterations.\n",
      "Optimal value function:\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -1.]\n",
      " [-1. -2. -1.  0.]\n",
      " [-2. -1.  0.  0.]]\n",
      "Optimal policy:\n",
      "T < < v\n",
      "^ ^ ^ v\n",
      "^ ^ v v\n",
      "^ > > T\n"
     ]
    }
   ],
   "source": [
    "# Full Policy Iteration\n",
    "policy = { (i,j): [0.25,0.25,0.25,0.25] for i in range(rows) for j in range(cols) if not is_terminal((i,j)) }\n",
    "V = np.zeros((rows, cols))\n",
    "\n",
    "stable = False # This flag will track if the policy has converged\n",
    "iteration = 0 # This counter will track the number of iterations\n",
    "while not stable:\n",
    "    iteration += 1\n",
    "    # Policy Evaluation (simpler approach: a finite number of sweeps or until stable)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                s = (i,j)\n",
    "                if is_terminal(s): \n",
    "                    continue\n",
    "                v = V[s]\n",
    "                new_v = 0\n",
    "                for idx, action in enumerate(actions):\n",
    "                    prob = policy[s][idx]\n",
    "                    next_s, reward = step(s, action)\n",
    "                    new_v += prob * (reward + gamma * V[next_s])\n",
    "                V[s] = new_v\n",
    "                delta = max(delta, abs(v - new_v))\n",
    "        if delta < 1e-4:\n",
    "            break\n",
    "\n",
    "    # Policy Improvement\n",
    "    stable = True\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            s = (i,j)\n",
    "            if is_terminal(s): \n",
    "                continue\n",
    "            # current policy action (deterministic for simplicity: pick first with prob)\n",
    "            current_action = np.argmax(policy[s])\n",
    "            # find best action via one-step lookahead\n",
    "            q_values = []\n",
    "            for action in actions:\n",
    "                next_s, reward = step(s, action)\n",
    "                q_values.append(reward + gamma * V[next_s])\n",
    "            best_action = np.argmax(q_values)\n",
    "            # If policy action is not best action, update policy\n",
    "            if best_action != current_action: \n",
    "                stable = False\n",
    "            # make new policy greedy (deterministic)\n",
    "            policy[s] = [0.0]*4\n",
    "            policy[s][best_action] = 1.0\n",
    "    if stable:\n",
    "        print(f\"Policy iteration converged in {iteration} iterations.\")\n",
    "        break\n",
    "\n",
    "# Final optimal value function and policy\n",
    "print(\"Optimal value function:\")\n",
    "print(V)\n",
    "print(\"Optimal policy:\")\n",
    "for i in range(rows):\n",
    "    row_policy = []\n",
    "    for j in range(cols):\n",
    "        if is_terminal((i,j)):\n",
    "            row_policy.append(\"T\")\n",
    "        else:\n",
    "            act = np.argmax(policy[(i,j)])\n",
    "            arrow = {0:\"^\", 1:\"v\", 2:\"<\", 3:\">\"}[act]\n",
    "            row_policy.append(arrow)\n",
    "    print(\" \".join(row_policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa79590a-b9f0-40f2-b609-929ed7794d91",
   "metadata": {},
   "source": [
    "Lets break down the policy iteration algorithm. There are two main loops:\n",
    "1. Outer Loop: Policy Iteration (continues until policy is stable)\n",
    "2. Inner Loop: Policy Evaluation (iterates until value function converges)\n",
    "\n",
    "Inner Convergence (Policy Evaluation): Values stabilize when the maximum change in any state's value (delta) is below threshold (1e-4)\n",
    "\n",
    "Outer Convergence (Policy Iteration): Policy converges when no changes are made to the policy during an evaluation-improvement cycle.\n",
    "\n",
    "This should converge in a few iterations. The optimal policy will direct the agent to the nearest terminal in the minimum number of steps (essentially a shortest path to either (0,0) or (3,3)). The optimal value function will show the negative of the distance to a terminal (since each step costs -1). For example, the value of a cell might equal $-d$ where d is the Manhattan distance to the closest terminal.\n",
    "\n",
    "**IMPORTANT:**\n",
    "A common source of bugs is actions in the policy that are not defined in the same order as the actions list. For example, if you use `np.argmax(policy[s])` to get the current action, and the actions list is `[0, 1, 2, 3]`, but the policy is defined as a dictionary with arbitrary key order like `{3: 0.25, 1: 0.25, 0: 0.25, 2: 0.25}`, then the action will be incorrect since dictionaries don't maintain order.\n",
    "\n",
    "Here's a better way to structure this using an Enum or constants to ensure consistency:\n",
    "\n",
    "```python\n",
    "class Action(Enum):\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3\n",
    "```\n",
    "However, object oriented programming is outside the scope of this tutorial, so we won't go into more detail here.\n",
    "\n",
    "### Value Iteration\n",
    "\n",
    "We demonstrated policy iteration above. **Value Iteration** is another DP approach that skips having a separate policy evaluation step. It directly updates the value function with the best possible action at each step:\n",
    "$$V_{k+1}(s) = \\max_a \\sum_{s{\\prime},r} P(s{\\prime},r|s,a)[r + \\gamma V_k(s{\\prime})]$$\n",
    "This is essentially applying the Bellman optimality update repeatedly.\n",
    "\n",
    "Let\u2019s do value iteration for the same grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c988d97-3bb5-4ee8-819f-292a5fb32101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration converged in 3 sweeps.\n",
      "Optimal value function (from Value Iteration):\n",
      "[[ 0.  0. -1. -2.]\n",
      " [ 0. -1. -2. -1.]\n",
      " [-1. -2. -1.  0.]\n",
      " [-2. -1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Value Iteration\n",
    "V_vi = np.zeros((rows, cols))  # Start with all values at 0\n",
    "gamma = 1.0\n",
    "theta = 1e-4\n",
    "iters = 0\n",
    "while True:\n",
    "    iters += 1\n",
    "    delta = 0\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            s = (i,j)\n",
    "            if is_terminal(s):\n",
    "                continue\n",
    "            v = V_vi[s]  # Store current value for comparison later\n",
    "            # Bellman optimality backup\n",
    "            best_q = -float('inf') # Initialize worst possible value\n",
    "            # Compute the best possible action for this state\n",
    "            for action in actions:\n",
    "                next_s, reward = step(s, action)\n",
    "                q = reward + gamma * V_vi[next_s] # Bellman equation\n",
    "                if q > best_q:\n",
    "                    best_q = q\n",
    "            V_vi[s] = best_q # Update state value to best found\n",
    "            delta = max(delta, abs(v - best_q))  # Track biggest change\n",
    "    if delta < theta:\n",
    "        break # We're done!\n",
    "\n",
    "print(f\"Value Iteration converged in {iters} sweeps.\")\n",
    "print(\"Optimal value function (from Value Iteration):\")\n",
    "print(V_vi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1271b136-9f60-41d5-bf10-457b40393418",
   "metadata": {},
   "source": [
    "This should yield the same optimal value function as policy iteration (or very close). We can extract the policy by taking the argmax actions from this value function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eead019f-166b-4ada-816f-e69ff0f39e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy (from Value Iteration):\n",
      "T < < v\n",
      "^ ^ ^ v\n",
      "^ ^ v v\n",
      "^ > > T\n"
     ]
    }
   ],
   "source": [
    "optimal_policy = []\n",
    "for i in range(rows):\n",
    "    row_policy = []\n",
    "    for j in range(cols):\n",
    "        s = (i,j)\n",
    "        if is_terminal(s):\n",
    "            row_policy.append(\"T\")\n",
    "        else:\n",
    "            # choose best action for this state\n",
    "            best_action = None\n",
    "            best_q = -float('inf')\n",
    "            for idx, action in enumerate(actions):\n",
    "                next_s, reward = step(s, action)\n",
    "                q = reward + gamma * V_vi[next_s]\n",
    "                if q > best_q:\n",
    "                    best_q = q\n",
    "                    best_action = idx\n",
    "            arrow = {0:\"^\", 1:\"v\", 2:\"<\", 3:\">\"}[best_action]\n",
    "            row_policy.append(arrow)\n",
    "    optimal_policy.append(\" \".join(row_policy))\n",
    "\n",
    "print(\"Optimal policy (from Value Iteration):\")\n",
    "for row in optimal_policy:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a4383b-e78f-43da-b2bf-948a9d0f57c5",
   "metadata": {},
   "source": [
    "It should match the earlier optimal policy.\n",
    "\n",
    "However, value iteration is more efficient than policy iteration because it doesn't require separate policy evaluation steps. Policy Iteration has two nested loops (1) Outer loop for policy updates and (2) Inner loop for policy evaluation (which itself needs multiple sweeps). Value Iteration combines these into a single loop. Each sweep directly updates values towards optimality\n",
    "and converges to the optimal value function faster since it doesn't need to evaluate the intermediate policy.\n",
    "\n",
    "We used Dynamic Programming to compute an optimal policy for a known MDP:\n",
    "- **Policy Evaluation**: Computed value functions for a given policy by iterative updates until convergence\n",
    "- **Policy Improvement**: Greedily improved the policy using one-step lookahead on the value function\n",
    "- **Policy Iteration**: Repeated evaluation and improvement to reach an optimal policy\n",
    "- **Value Iteration**: Directly computed the optimal value function by iterative Bellman optimality updates\n",
    "\n",
    "DP methods guarantee finding the optimal policy if the model is known and the state space is not too large. However, they require full knowledge of transition probabilities and are computationally expensive for large problems. In model-free scenarios, we turn to learning methods like Monte Carlo and Temporal-Difference, which we\u2019ll explore next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n",
    "[\u2190 Part 2](../part-2-multi-armed-bandits/) | [All labs](../) | [Part 4 \u2192](../part-4-monte-carlo-methods/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}